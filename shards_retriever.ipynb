{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from copy import deepcopy\n",
    "from llms.llm_interact import LLMConfig\n",
    "from llms.retriever import Retriever\n",
    "from llm_interact_env import Environment, EnvironmentConfig, Task, run\n",
    "from logger import logger, configure_global_logger  # Import the logger\n",
    "import subprocess  # Added for running the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 01:12:27 - DataSciBench - INFO - Loaded configuration from llm_configs/raw_tianyu/llm_config_shards.toml\n",
      "2025-05-14 01:12:27 - DataSciBench - INFO - Initialized Gatekeeper with model: litellm_proxy/claude-3-7-sonnet, temperature: 1\n"
     ]
    }
   ],
   "source": [
    "retriever_config = LLMConfig.from_toml(\"llm_configs/raw_tianyu/llm_config_shards.toml\")\n",
    "\n",
    "retriever = Retriever(retriever_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 instruction files\n",
      "Processing: benchmark_final_test/storage/aarthi93-end-to-end-ml-pipeline/instructions/gatekeeper_reference.md\n",
      "Project: aarthi93-end-to-end-ml-pipeline\n",
      "Response: # Detailed Data Science Research Pipeline\n",
      "\n",
      "1. Review the Ames Housing dataset documentation to understand the feature definitions, expected ranges, and domain context before beginning any analysis.\n",
      "\n",
      "2. Import standard data science libraries including pandas, numpy, matplotlib, seaborn, and scikit-learn components (preprocessing tools, model selection utilities, ensemble methods, and metrics).\n",
      "\n",
      "3. Load the Ames Housing dataset CSV file into a pandas DataFrame and perform initial inspection using df.info(), df.head(), and df.describe() to understand basic data characteristics.\n",
      "\n",
      "4. Create a missing values heatmap or bar chart to visualize the percentage of missing values across all columns, identifying features with particularly high missingness.\n",
      "\n",
      "5. Analyze columns with high missing percentages (like Alley, Pool QC, Fence, Misc Feature) to determine if they should be imputed or dropped based on their potential predictive value.\n",
      "\n",
      "6. Remove unnecessary columns including any index columns, ID features, and those with missing values exceeding a reasonable threshold (typically >80%).\n",
      "\n",
      "7. Conduct exploratory data analysis on numerical features, examining distributions and correlations with the target variable (SalePrice) to inform feature engineering decisions.\n",
      "\n",
      "8. Explore categorical features through frequency counts and box plots of SalePrice by category to understand their impact on the target variable.\n",
      "\n",
      "9. Engineer temporal features by calculating house age (current year - YearBuilt) and years since remodeling (current year - YearRemodAdd).\n",
      "\n",
      "10. Create a total square footage feature by combining relevant area measurements (basement, first floor, second floor) based on correlation analysis with the target.\n",
      "\n",
      "11. Calculate a weighted total bathroom count by adding full bathrooms and applying a 0.5 weight to half bathrooms, reflecting their relative value.\n",
      "\n",
      "12. Generate a binary feature indicating whether a property has a pool based on the Pool Area feature, potentially simplifying the relationship with price.\n",
      "\n",
      "13. Validate each engineered feature by examining its relationship with SalePrice and remove original features used in the engineering process to avoid redundancy.\n",
      "\n",
      "14. Check the distribution of the target variable (SalePrice) and consider transformations if significant skew is detected.\n",
      "\n",
      "15. Split the dataset into training (80%) and test (20%) sets using a fixed random seed for reproducibility.\n",
      "\n",
      "16. Create separate preprocessing pipelines for numerical features (median imputation followed by standard scaling) and categorical features (constant 'None' imputation followed by one-hot encoding).\n",
      "\n",
      "17. Combine the numerical and categorical preprocessing steps using a ColumnTransformer to create a unified preprocessing pipeline.\n",
      "\n",
      "18. Test multiple model types (Linear Regression, Random Forest, Gradient Boosting) on a small subset to identify promising candidates.\n",
      "\n",
      "19. Perform cross-validation with the Random Forest model, testing different hyperparameter values to optimize performance.\n",
      "\n",
      "20. Train the final Random Forest Regressor with optimal hyperparameters (n_estimators=150, max_depth=30, min_samples_split=5) using the complete preprocessing pipeline.\n",
      "\n",
      "21. Generate predictions on the test set and calculate multiple evaluation metrics (RMSE, MAE, RÂ²) to assess model performance from different perspectives.\n",
      "\n",
      "22. Create visualizations comparing predicted vs. actual home prices to identify patterns in prediction errors and potential model improvements.\n",
      "--------------------------------------------------\n",
      "Saved reference insights to: benchmark_final_test/storage/aarthi93-end-to-end-ml-pipeline/instructions/shards.md\n",
      "Processing: benchmark_final_test/storage/abdallaellaithy-titanic-in-space-ml-survival-predictions/instructions/gatekeeper_reference.md\n",
      "Project: abdallaellaithy-titanic-in-space-ml-survival-predictions\n",
      "Response: # Detailed Data Science Pipeline for Spaceship Titanic Classification\n",
      "\n",
      "1. Import essential data manipulation libraries (pandas, numpy), visualization libraries (matplotlib, seaborn), and basic machine learning tools (scikit-learn) to begin exploratory analysis.\n",
      "\n",
      "2. Import specialized modeling libraries (XGBoost, LightGBM) for gradient boosting implementation, ensuring all dependencies are properly installed.\n",
      "\n",
      "3. Load the training and test datasets from their specified paths, examining the returned shapes to verify successful data loading.\n",
      "\n",
      "4. Perform initial exploratory data analysis (EDA) to understand the structure of the data - check column types, basic statistics, and the distribution of the target variable 'Transported'.\n",
      "\n",
      "5. Investigate missing values across all features in both datasets to determine appropriate handling strategies for each column.\n",
      "\n",
      "6. Examine the 'Cabin' column structure to understand its components before attempting any feature extraction.\n",
      "\n",
      "7. Parse the 'Cabin' column to extract meaningful components: Deck (letter), Number (numeric value), and Side (P/S indicator) based on the observed pattern.\n",
      "\n",
      "8. Analyze the 'PassengerId' format to understand how passenger grouping information is encoded within this identifier.\n",
      "\n",
      "9. Extract group information from 'PassengerId' using string manipulation, creating a new feature to identify passengers traveling together.\n",
      "\n",
      "10. Create an 'IsAlone' binary flag to indicate passengers traveling without companions, which could be a meaningful predictor for survival.\n",
      "\n",
      "11. Investigate spending patterns by analyzing expense-related columns (RoomService, FoodCourt, ShoppingMall, Spa, VRDeck).\n",
      "\n",
      "12. Engineer a 'TotalSpent' feature by summing all spending categories and create a binary 'HasSpent' indicator for any non-zero spending.\n",
      "\n",
      "13. Fill missing spending values with zeros, assuming that missing values indicate no spending in that category.\n",
      "\n",
      "14. Combine train and test datasets to ensure consistent preprocessing, then apply all feature engineering steps to the combined data.\n",
      "\n",
      "15. Split the processed combined data back into training and test sets for model development.\n",
      "\n",
      "16. Analyze feature correlations and distributions to better understand relationships within the data and with the target variable.\n",
      "\n",
      "17. Create separate preprocessing pipelines for numerical and categorical features, testing different imputation strategies for each type.\n",
      "\n",
      "18. For numerical features, implement a pipeline with median imputation followed by standardization to handle missing values and scale features.\n",
      "\n",
      "19. For categorical features, test multiple approaches: most frequent imputation with one-hot encoding versus creating missing value categories.\n",
      "\n",
      "20. Build a composite preprocessing pipeline using ColumnTransformer to apply the appropriate transformations to each feature type.\n",
      "\n",
      "21. Define the feature set by dropping redundant or unnecessary columns, and establish the target variable for supervised learning.\n",
      "\n",
      "22. Split the training data into training and validation sets using stratification to maintain class balance across splits.\n",
      "\n",
      "23. Create baseline models using multiple algorithms (Random Forest, Gradient Boosting, XGBoost, LightGBM) with default parameters to establish performance benchmarks.\n",
      "\n",
      "24. Define an evaluation function that calculates and reports relevant metrics (accuracy and ROC AUC) for classification model comparison.\n",
      "\n",
      "25. Evaluate all baseline models on the validation set, storing results in a structured format for comparison.\n",
      "\n",
      "26. Select the best-performing model based on ROC AUC scores from the baseline evaluation.\n",
      "\n",
      "27. Define model-specific hyperparameter grids focusing on key parameters like tree depth, learning rate, and number of estimators.\n",
      "\n",
      "28. Implement hyperparameter tuning using GridSearchCV with 5-fold cross-validation on the best model to optimize performance.\n",
      "\n",
      "29. Extract and analyze the best hyperparameter configuration and corresponding performance metrics from the grid search results.\n",
      "\n",
      "30. Evaluate the tuned model on the validation set to measure performance improvement over the baseline model.\n",
      "\n",
      "31. Train the final model with optimized hyperparameters on the full training dataset to maximize learning from available data.\n",
      "\n",
      "32. Generate predictions on the test dataset using the final tuned model, formatting results according to submission requirements.\n",
      "\n",
      "33. Create a submission dataframe with PassengerId and predicted Transported values, ensuring proper boolean type conversion.\n",
      "\n",
      "34. Save the submission file in the required format for competition evaluation.\n",
      "--------------------------------------------------\n",
      "Saved reference insights to: benchmark_final_test/storage/abdallaellaithy-titanic-in-space-ml-survival-predictions/instructions/shards.md\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "# Find all instruction.md files in the storage directory\n",
    "storage_dir = \"benchmark_final_test/storage\"\n",
    "instruction_files = []\n",
    "\n",
    "# Walk through all directories under storage\n",
    "for root, dirs, files in os.walk(storage_dir):\n",
    "    # Check if this is an 'instructions' directory\n",
    "    if os.path.basename(root) == \"instructions\":\n",
    "        # Look for instructions.md file\n",
    "        instruction_file = os.path.join(root, \"gatekeeper_reference.md\")\n",
    "        if os.path.exists(instruction_file):\n",
    "            instruction_files.append(instruction_file)\n",
    "\n",
    "print(f\"Found {len(instruction_files)} instruction files\")\n",
    "\n",
    "# Process each instruction file\n",
    "for instruction_file in instruction_files:\n",
    "    print(f\"Processing: {instruction_file}\")\n",
    "    \n",
    "    # Load the instruction content\n",
    "    with open(instruction_file, \"r\") as f:\n",
    "        instruction_content = f.read()\n",
    "    \n",
    "    # Process the instruction content\n",
    "    modified_content = instruction_content\n",
    "    \n",
    "    # If the first line contains \"submission.csv\", remove it and the following newline\n",
    "    if \"submission.csv\" in modified_content.split('\\n')[0]:\n",
    "        modified_content = '\\n'.join(modified_content.split('\\n')[1:])\n",
    "    \n",
    "    # Replace double newlines with single newlines\n",
    "    modified_content = modified_content.replace('\\n\\n', '\\n')\n",
    "    \n",
    "    # Call the retriever with the modified instruction content\n",
    "    response = retriever.call_llm(modified_content, thinking={\"type\": \"enabled\", \"budget_tokens\": 8196})\n",
    "    # Print the project name and response\n",
    "    project_name = Path(instruction_file).parts[-3]  # Get the project name from the path\n",
    "    print(f\"Project: {project_name}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Save the response to reference_insights file in the same directory as instructions.md\n",
    "    shards_path = os.path.join(os.path.dirname(instruction_file), \"shards.md\")\n",
    "    with open(shards_path, \"w\") as f:\n",
    "        # If the response starts with #, remove the first line\n",
    "        if response.startswith(\"#\"):\n",
    "            response = \"\\n\".join(response.split(\"\\n\")[1:])\n",
    "        f.write(response)\n",
    "    print(f\"Saved reference insights to: {shards_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
