{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from copy import deepcopy\n",
    "from llms.llm_interact import LLMConfig\n",
    "from llms.retriever import Retriever\n",
    "from llm_interact_env import Environment, EnvironmentConfig, Task, run\n",
    "from logger import logger, configure_global_logger  # Import the logger\n",
    "import subprocess  # Added for running the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 01:12:27 - DataSciBench - INFO - Loaded configuration from llm_configs/raw_tianyu/llm_config_shards.toml\n",
      "2025-05-14 01:12:27 - DataSciBench - INFO - Initialized Gatekeeper with model: litellm_proxy/claude-3-7-sonnet, temperature: 1\n"
     ]
    }
   ],
   "source": [
    "retriever_config = LLMConfig.from_toml(\"llm_configs/raw_tianyu/llm_config_shards.toml\")\n",
    "\n",
    "retriever = Retriever(retriever_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 instruction files\n",
      "Processing: benchmark_final_test/storage/aarthi93-end-to-end-ml-pipeline/instructions/gatekeeper_reference.md\n",
      "Project: aarthi93-end-to-end-ml-pipeline\n",
      "Response: # Detailed Data Science Research Pipeline\n",
      "\n",
      "1. Review the Ames Housing dataset documentation to understand the feature definitions, expected ranges, and domain context before beginning any analysis.\n",
      "\n",
      "2. Import standard data science libraries including pandas, numpy, matplotlib, seaborn, and scikit-learn components (preprocessing tools, model selection utilities, ensemble methods, and metrics).\n",
      "\n",
      "3. Load the Ames Housing dataset CSV file into a pandas DataFrame and perform initial inspection using df.info(), df.head(), and df.describe() to understand basic data characteristics.\n",
      "\n",
      "4. Create a missing values heatmap or bar chart to visualize the percentage of missing values across all columns, identifying features with particularly high missingness.\n",
      "\n",
      "5. Analyze columns with high missing percentages (like Alley, Pool QC, Fence, Misc Feature) to determine if they should be imputed or dropped based on their potential predictive value.\n",
      "\n",
      "6. Remove unnecessary columns including any index columns, ID features, and those with missing values exceeding a reasonable threshold (typically >80%).\n",
      "\n",
      "7. Conduct exploratory data analysis on numerical features, examining distributions and correlations with the target variable (SalePrice) to inform feature engineering decisions.\n",
      "\n",
      "8. Explore categorical features through frequency counts and box plots of SalePrice by category to understand their impact on the target variable.\n",
      "\n",
      "9. Engineer temporal features by calculating house age (current year - YearBuilt) and years since remodeling (current year - YearRemodAdd).\n",
      "\n",
      "10. Create a total square footage feature by combining relevant area measurements (basement, first floor, second floor) based on correlation analysis with the target.\n",
      "\n",
      "11. Calculate a weighted total bathroom count by adding full bathrooms and applying a 0.5 weight to half bathrooms, reflecting their relative value.\n",
      "\n",
      "12. Generate a binary feature indicating whether a property has a pool based on the Pool Area feature, potentially simplifying the relationship with price.\n",
      "\n",
      "13. Validate each engineered feature by examining its relationship with SalePrice and remove original features used in the engineering process to avoid redundancy.\n",
      "\n",
      "14. Check the distribution of the target variable (SalePrice) and consider transformations if significant skew is detected.\n",
      "\n",
      "15. Split the dataset into training (80%) and test (20%) sets using a fixed random seed for reproducibility.\n",
      "\n",
      "16. Create separate preprocessing pipelines for numerical features (median imputation followed by standard scaling) and categorical features (constant 'None' imputation followed by one-hot encoding).\n",
      "\n",
      "17. Combine the numerical and categorical preprocessing steps using a ColumnTransformer to create a unified preprocessing pipeline.\n",
      "\n",
      "18. Test multiple model types (Linear Regression, Random Forest, Gradient Boosting) on a small subset to identify promising candidates.\n",
      "\n",
      "19. Perform cross-validation with the Random Forest model, testing different hyperparameter values to optimize performance.\n",
      "\n",
      "20. Train the final Random Forest Regressor with optimal hyperparameters (n_estimators=150, max_depth=30, min_samples_split=5) using the complete preprocessing pipeline.\n",
      "\n",
      "21. Generate predictions on the test set and calculate multiple evaluation metrics (RMSE, MAE, RÂ²) to assess model performance from different perspectives.\n",
      "\n",
      "22. Create visualizations comparing predicted vs. actual home prices to identify patterns in prediction errors and potential model improvements.\n",
      "--------------------------------------------------\n",
      "Saved reference insights to: benchmark_final_test/storage/aarthi93-end-to-end-ml-pipeline/instructions/shards.md\n",
      "Processing: benchmark_final_test/storage/abdallaellaithy-titanic-in-space-ml-survival-predictions/instructions/gatekeeper_reference.md\n",
      "Project: abdallaellaithy-titanic-in-space-ml-survival-predictions\n",
      "Response: # Detailed Data Science Pipeline for Spaceship Titanic Classification\n",
      "\n",
      "1. Import essential data manipulation libraries (pandas, numpy), visualization libraries (matplotlib, seaborn), and basic machine learning tools (scikit-learn) to begin exploratory analysis.\n",
      "\n",
      "2. Import specialized modeling libraries (XGBoost, LightGBM) for gradient boosting implementation, ensuring all dependencies are properly installed.\n",
      "\n",
      "3. Load the training and test datasets from their specified paths, examining the returned shapes to verify successful data loading.\n",
      "\n",
      "4. Perform initial exploratory data analysis (EDA) to understand the structure of the data - check column types, basic statistics, and the distribution of the target variable 'Transported'.\n",
      "\n",
      "5. Investigate missing values across all features in both datasets to determine appropriate handling strategies for each column.\n",
      "\n",
      "6. Examine the 'Cabin' column structure to understand its components before attempting any feature extraction.\n",
      "\n",
      "7. Parse the 'Cabin' column to extract meaningful components: Deck (letter), Number (numeric value), and Side (P/S indicator) based on the observed pattern.\n",
      "\n",
      "8. Analyze the 'PassengerId' format to understand how passenger grouping information is encoded within this identifier.\n",
      "\n",
      "9. Extract group information from 'PassengerId' using string manipulation, creating a new feature to identify passengers traveling together.\n",
      "\n",
      "10. Create an 'IsAlone' binary flag to indicate passengers traveling without companions, which could be a meaningful predictor for survival.\n",
      "\n",
      "11. Investigate spending patterns by analyzing expense-related columns (RoomService, FoodCourt, ShoppingMall, Spa, VRDeck).\n",
      "\n",
      "12. Engineer a 'TotalSpent' feature by summing all spending categories and create a binary 'HasSpent' indicator for any non-zero spending.\n",
      "\n",
      "13. Fill missing spending values with zeros, assuming that missing values indicate no spending in that category.\n",
      "\n",
      "14. Combine train and test datasets to ensure consistent preprocessing, then apply all feature engineering steps to the combined data.\n",
      "\n",
      "15. Split the processed combined data back into training and test sets for model development.\n",
      "\n",
      "16. Analyze feature correlations and distributions to better understand relationships within the data and with the target variable.\n",
      "\n",
      "17. Create separate preprocessing pipelines for numerical and categorical features, testing different imputation strategies for each type.\n",
      "\n",
      "18. For numerical features, implement a pipeline with median imputation followed by standardization to handle missing values and scale features.\n",
      "\n",
      "19. For categorical features, test multiple approaches: most frequent imputation with one-hot encoding versus creating missing value categories.\n",
      "\n",
      "20. Build a composite preprocessing pipeline using ColumnTransformer to apply the appropriate transformations to each feature type.\n",
      "\n",
      "21. Define the feature set by dropping redundant or unnecessary columns, and establish the target variable for supervised learning.\n",
      "\n",
      "22. Split the training data into training and validation sets using stratification to maintain class balance across splits.\n",
      "\n",
      "23. Create baseline models using multiple algorithms (Random Forest, Gradient Boosting, XGBoost, LightGBM) with default parameters to establish performance benchmarks.\n",
      "\n",
      "24. Define an evaluation function that calculates and reports relevant metrics (accuracy and ROC AUC) for classification model comparison.\n",
      "\n",
      "25. Evaluate all baseline models on the validation set, storing results in a structured format for comparison.\n",
      "\n",
      "26. Select the best-performing model based on ROC AUC scores from the baseline evaluation.\n",
      "\n",
      "27. Define model-specific hyperparameter grids focusing on key parameters like tree depth, learning rate, and number of estimators.\n",
      "\n",
      "28. Implement hyperparameter tuning using GridSearchCV with 5-fold cross-validation on the best model to optimize performance.\n",
      "\n",
      "29. Extract and analyze the best hyperparameter configuration and corresponding performance metrics from the grid search results.\n",
      "\n",
      "30. Evaluate the tuned model on the validation set to measure performance improvement over the baseline model.\n",
      "\n",
      "31. Train the final model with optimized hyperparameters on the full training dataset to maximize learning from available data.\n",
      "\n",
      "32. Generate predictions on the test dataset using the final tuned model, formatting results according to submission requirements.\n",
      "\n",
      "33. Create a submission dataframe with PassengerId and predicted Transported values, ensuring proper boolean type conversion.\n",
      "\n",
      "34. Save the submission file in the required format for competition evaluation.\n",
      "--------------------------------------------------\n",
      "Saved reference insights to: benchmark_final_test/storage/abdallaellaithy-titanic-in-space-ml-survival-predictions/instructions/shards.md\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "# Find all instruction.md files in the storage directory\n",
    "storage_dir = \"benchmark_final_test/storage\"\n",
    "instruction_files = []\n",
    "\n",
    "# Walk through all directories under storage\n",
    "for root, dirs, files in os.walk(storage_dir):\n",
    "    # Check if this is an 'instructions' directory\n",
    "    if os.path.basename(root) == \"instructions\":\n",
    "        # Look for instructions.md file\n",
    "        instruction_file = os.path.join(root, \"gatekeeper_reference.md\")\n",
    "        if os.path.exists(instruction_file):\n",
    "            instruction_files.append(instruction_file)\n",
    "\n",
    "print(f\"Found {len(instruction_files)} instruction files\")\n",
    "\n",
    "# Process each instruction file\n",
    "for instruction_file in instruction_files:\n",
    "    print(f\"Processing: {instruction_file}\")\n",
    "    \n",
    "    # Load the instruction content\n",
    "    with open(instruction_file, \"r\") as f:\n",
    "        instruction_content = f.read()\n",
    "    \n",
    "    # Process the instruction content\n",
    "    modified_content = instruction_content\n",
    "    \n",
    "    # If the first line contains \"submission.csv\", remove it and the following newline\n",
    "    if \"submission.csv\" in modified_content.split('\\n')[0]:\n",
    "        modified_content = '\\n'.join(modified_content.split('\\n')[1:])\n",
    "    \n",
    "    # Replace double newlines with single newlines\n",
    "    modified_content = modified_content.replace('\\n\\n', '\\n')\n",
    "    \n",
    "    # Call the retriever with the modified instruction content\n",
    "    response = retriever.call_llm(modified_content, thinking={\"type\": \"enabled\", \"budget_tokens\": 8196})\n",
    "    # Print the project name and response\n",
    "    project_name = Path(instruction_file).parts[-3]  # Get the project name from the path\n",
    "    print(f\"Project: {project_name}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Save the response to reference_insights file in the same directory as instructions.md\n",
    "    shards_path = os.path.join(os.path.dirname(instruction_file), \"shards.md\")\n",
    "    with open(shards_path, \"w\") as f:\n",
    "        # If the response starts with #, remove the first line\n",
    "        if response.startswith(\"#\"):\n",
    "            response = \"\\n\".join(response.split(\"\\n\")[1:])\n",
    "        f.write(response)\n",
    "    print(f\"Saved reference insights to: {shards_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 01:56:53 - DataSciBench - INFO - Loaded configuration from llm_configs/raw_tianyu/llm_config_shards.toml\n",
      "2025-05-14 01:56:53 - DataSciBench - INFO - Initialized Gatekeeper with model: litellm_proxy/claude-3-7-sonnet, temperature: 1\n",
      "Found 21 instruction files\n",
      "Processing: benchmark_final/storage/ugurcan95-brazilian-tweet-sentiment-analysis/instructions/cleaned_instructions.md\n",
      "Project: ugurcan95-brazilian-tweet-sentiment-analysis\n",
      "Response: # Detailed NLP Pipeline for Portuguese Tweet Sentiment Analysis\n",
      "\n",
      "1. Perform exploratory data analysis on the tweet dataset, examining tweet length distribution, language characteristics, common tokens, and sentiment class distribution to inform subsequent preprocessing decisions.\n",
      "\n",
      "2. Create a text cleaning function with modular components for each cleaning operation (lowercase conversion, URL removal, mention stripping, etc.), applying each transformation sequentially and testing their impact on a sample of tweets.\n",
      "\n",
      "3. Define a comprehensive text cleaning pipeline that combines all necessary operations: lowercase conversion, removal of URLs, retweet indicators, hashtags, mentions, punctuation, digits, special characters, HTML entities, and emojis.\n",
      "\n",
      "4. Apply the complete text cleaning function to the tweet_text column and verify the quality of cleaned text through random sampling.\n",
      "\n",
      "5. Explore Portuguese stopword lists from different libraries (NLTK, spaCy) and compare their coverage and appropriateness for social media text.\n",
      "\n",
      "6. Create a function to remove Portuguese stopwords from each tweet while preserving words that carry sentiment information.\n",
      "\n",
      "7. Research lemmatization tools that support Portuguese language and evaluate their performance on sample tweets.\n",
      "\n",
      "8. Implement a POS-tagging based lemmatization helper function that processes individual words according to their grammatical role.\n",
      "\n",
      "9. Develop a document-level lemmatization function that tokenizes sentences, removes punctuation, tags words with POS information, and applies the appropriate lemmatization.\n",
      "\n",
      "10. Apply the lemmatization pipeline to the cleaned tweet_text and evaluate its effectiveness in reducing vocabulary size while maintaining semantic meaning.\n",
      "\n",
      "11. Split the data into training and validation sets with stratification to ensure balanced sentiment distribution across splits.\n",
      "\n",
      "12. Test different feature extraction approaches (CountVectorizer, TF-IDF) with various n-gram ranges on the validation set to determine the optimal configuration.\n",
      "\n",
      "13. Implement TF-IDF vectorization with unigrams and bigrams, tuning parameters like min_df and max_df based on corpus characteristics.\n",
      "\n",
      "14. Extract the target variable (sentiment) from the dataframe and verify its encoding is appropriate for classification (e.g., converted to numeric labels if needed).\n",
      "\n",
      "15. Benchmark multiple classification algorithms (Logistic Regression, SVM, Naive Bayes) on a small validation set to determine the most promising approach.\n",
      "\n",
      "16. Train a Logistic Regression model with increased max_iterations and n_jobs parameters for parallel processing, incorporating class weights if sentiment distribution is imbalanced.\n",
      "\n",
      "17. Evaluate the trained model using accuracy, precision, recall, F1-score, and confusion matrix to understand performance across different sentiment classes.\n",
      "\n",
      "18. Process the test dataset using the identical preprocessing pipeline: text cleaning, stopword removal, and lemmatization to ensure consistency.\n",
      "\n",
      "19. Transform the preprocessed test data using the same TF-IDF vectorizer previously fitted on the training data.\n",
      "\n",
      "20. Generate predictions on the test set using the trained Logistic Regression model and prepare results in the required submission format.\n",
      "--------------------------------------------------\n",
      "Saved reference insights to: benchmark_final/storage/ugurcan95-brazilian-tweet-sentiment-analysis/instructions/shards.md\n",
      "Processing: benchmark_final/storage/mightyjiraiya-titanic-survival-prediction/instructions/cleaned_instructions.md\n",
      "Project: mightyjiraiya-titanic-survival-prediction\n",
      "Response: # Detailed Titanic Survival Prediction Pipeline\n",
      "\n",
      "1. Perform exploratory data analysis to understand feature distributions, correlations with survival, and missing value patterns across both datasets.\n",
      "\n",
      "2. Analyze passenger names by examining their structure, frequency patterns, and relationship with survival rates before attempting title extraction.\n",
      "\n",
      "3. Extract titles from passenger names using regex patterns and group them into meaningful categories (Mr, Mrs, Miss, Master, etc.) based on frequency and semantic similarity.\n",
      "\n",
      "4. Calculate family size by combining SibSp and Parch features and adding 1 for the passenger themselves.\n",
      "\n",
      "5. Create an IsAlone flag to identify passengers traveling without family (family size = 1).\n",
      "\n",
      "6. Develop family group categories (Singleton, Small Family, Large Family) based on the distribution of family sizes and their survival rates.\n",
      "\n",
      "7. Examine cabin information structure and missing patterns to determine what meaningful features can be extracted.\n",
      "\n",
      "8. Create a HasCabin binary flag to distinguish passengers with recorded cabin information from those without.\n",
      "\n",
      "9. Extract deck information from the first letter of cabin values when available.\n",
      "\n",
      "10. Analyze ticket formats to identify patterns and determine what useful information can be extracted from this field.\n",
      "\n",
      "11. Process ticket information by extracting prefixes and converting numeric portions to features where meaningful.\n",
      "\n",
      "12. Examine age and fare distributions to identify natural breakpoints for binning that might reveal non-linear relationships with survival.\n",
      "\n",
      "13. Create age categories (Child, Young Adult, Adult, Senior) and fare bins based on distribution analysis and domain knowledge.\n",
      "\n",
      "14. Develop interaction features by combining important variables (Age*Class, Fare per person) to capture more complex relationships with survival.\n",
      "\n",
      "15. Consolidate all feature engineering steps into a comprehensive function that can be applied consistently to both training and test datasets.\n",
      "\n",
      "16. Analyze missing values in detail to understand patterns and potential biases before determining appropriate imputation strategies.\n",
      "\n",
      "17. Impute missing Age values using median ages calculated specifically for each Title, Sex, and Pclass group combination.\n",
      "\n",
      "18. Fill missing Embarked values with the mode after analyzing the few missing entries for potential patterns.\n",
      "\n",
      "19. Handle missing Fare values in the test set by calculating and applying median fares by Pclass from the training data.\n",
      "\n",
      "20. Recalculate age and fare bins after imputation to ensure all values are properly categorized.\n",
      "\n",
      "21. Evaluate feature importance using preliminary models to select the most relevant features for final prediction.\n",
      "\n",
      "22. Organize selected features into numeric and categorical groups for appropriate preprocessing.\n",
      "\n",
      "23. Create separate preprocessing pipelines with suitable transformations for each feature type (scaling for numeric, encoding for categorical).\n",
      "\n",
      "24. Implement a ColumnTransformer to apply the correct preprocessing to each feature type within a unified pipeline.\n",
      "\n",
      "25. Build an initial model comparison framework with multiple algorithms (Logistic Regression, Random Forest, Gradient Boosting, SVC).\n",
      "\n",
      "26. Configure cross-validation using StratifiedKFold to maintain class distribution across folds.\n",
      "\n",
      "27. Evaluate all models using cross-validation and identify the most promising candidates for hyperparameter tuning.\n",
      "\n",
      "28. Define focused parameter grids for top-performing models based on their preliminary results and model characteristics.\n",
      "\n",
      "29. Perform hyperparameter optimization using GridSearchCV with cross-validation to find optimal configurations.\n",
      "\n",
      "30. Analyze tuning results and select the best-performing models for potential ensemble creation.\n",
      "\n",
      "31. Create a VotingClassifier ensemble combining the best-tuned models with soft voting to leverage probability predictions.\n",
      "\n",
      "32. Evaluate the ensemble against individual models to confirm performance improvement.\n",
      "\n",
      "33. Train the final selected model on the complete training dataset.\n",
      "\n",
      "34. Generate survival predictions on the test dataset and format results according to submission requirements.\n",
      "--------------------------------------------------\n",
      "Saved reference insights to: benchmark_final/storage/mightyjiraiya-titanic-survival-prediction/instructions/shards.md\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from copy import deepcopy\n",
    "from llms.llm_interact import LLMConfig\n",
    "from llms.retriever import Retriever\n",
    "from llm_interact_env import Environment, EnvironmentConfig, Task, run\n",
    "from logger import logger, configure_global_logger  # Import the logger\n",
    "import subprocess  # Added for running the script\n",
    "\n",
    "\n",
    "retriever_config = LLMConfig.from_toml(\"llm_configs/raw_tianyu/llm_config_shards.toml\")\n",
    "\n",
    "retriever = Retriever(retriever_config)\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "# Find all instruction.md files in the storage directory\n",
    "storage_dir = \"benchmark_final/storage\"\n",
    "instruction_files = []\n",
    "\n",
    "# Walk through all directories under storage\n",
    "for root, dirs, files in os.walk(storage_dir):\n",
    "    # Check if this is an 'instructions' directory\n",
    "    if os.path.basename(root) == \"instructions\":\n",
    "        # Look for instructions.md file\n",
    "        try:\n",
    "            instruction_file = os.path.join(root, \"cleaned_instructions.md\")\n",
    "            if os.path.exists(instruction_file):\n",
    "                instruction_files.append(instruction_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {root}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Found {len(instruction_files)} instruction files\")\n",
    "\n",
    "# Process each instruction file\n",
    "for instruction_file in instruction_files[:2]:\n",
    "    print(f\"Processing: {instruction_file}\")\n",
    "    \n",
    "    # Load the instruction content\n",
    "    with open(instruction_file, \"r\") as f:\n",
    "        instruction_content = f.read()\n",
    "    \n",
    "    # Remove content between **Objective** and **Your Knowledge**\n",
    "    if \"**Objective**\" in instruction_content and \"**Your Knowledge**\" in instruction_content:\n",
    "        start_idx = instruction_content.find(\"**Objective**\")\n",
    "        end_idx = instruction_content.find(\"**Your Knowledge**\")\n",
    "        if start_idx != -1 and end_idx != -1:\n",
    "            # Keep the \"**Your Knowledge**\" part\n",
    "            modified_content = instruction_content[:start_idx] + instruction_content[end_idx:]\n",
    "        else:\n",
    "            modified_content = instruction_content\n",
    "    else:\n",
    "        modified_content = instruction_content\n",
    "    \n",
    "    # Call the retriever with the modified instruction content\n",
    "    response = retriever.call_llm(modified_content, thinking={\"type\": \"enabled\", \"budget_tokens\": 20000})\n",
    "    # Print the project name and response\n",
    "    project_name = Path(instruction_file).parts[-3]  # Get the project name from the path\n",
    "    print(f\"Project: {project_name}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Save the response to reference_insights file in the same directory as instructions.md\n",
    "    shards_path = os.path.join(os.path.dirname(instruction_file), \"shards.md\")\n",
    "    with open(shards_path, \"w\") as f:\n",
    "        # If the response starts with #, remove the first line\n",
    "        if response.startswith(\"#\"):\n",
    "            response = \"\\n\".join(response.split(\"\\n\")[1:])\n",
    "        f.write(response)\n",
    "    print(f\"Saved reference insights to: {shards_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
