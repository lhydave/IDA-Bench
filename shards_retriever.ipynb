{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from copy import deepcopy\n",
    "from llms.llm_interact import LLMConfig\n",
    "from llms.retriever import Retriever\n",
    "from llm_interact_env import Environment, EnvironmentConfig, Task, run\n",
    "from logger import logger, configure_global_logger  # Import the logger\n",
    "import subprocess  # Added for running the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 01:12:27 - DataSciBench - INFO - Loaded configuration from llm_configs/raw_tianyu/llm_config_shards.toml\n",
      "2025-05-14 01:12:27 - DataSciBench - INFO - Initialized Gatekeeper with model: litellm_proxy/claude-3-7-sonnet, temperature: 1\n"
     ]
    }
   ],
   "source": [
    "retriever_config = LLMConfig.from_toml(\"llm_configs/raw_tianyu/llm_config_shards.toml\")\n",
    "\n",
    "retriever = Retriever(retriever_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 instruction files\n",
      "Processing: benchmark_final_test/storage/aarthi93-end-to-end-ml-pipeline/instructions/gatekeeper_reference.md\n",
      "Project: aarthi93-end-to-end-ml-pipeline\n",
      "Response: # Detailed Data Science Research Pipeline\n",
      "\n",
      "1. Review the Ames Housing dataset documentation to understand the feature definitions, expected ranges, and domain context before beginning any analysis.\n",
      "\n",
      "2. Import standard data science libraries including pandas, numpy, matplotlib, seaborn, and scikit-learn components (preprocessing tools, model selection utilities, ensemble methods, and metrics).\n",
      "\n",
      "3. Load the Ames Housing dataset CSV file into a pandas DataFrame and perform initial inspection using df.info(), df.head(), and df.describe() to understand basic data characteristics.\n",
      "\n",
      "4. Create a missing values heatmap or bar chart to visualize the percentage of missing values across all columns, identifying features with particularly high missingness.\n",
      "\n",
      "5. Analyze columns with high missing percentages (like Alley, Pool QC, Fence, Misc Feature) to determine if they should be imputed or dropped based on their potential predictive value.\n",
      "\n",
      "6. Remove unnecessary columns including any index columns, ID features, and those with missing values exceeding a reasonable threshold (typically >80%).\n",
      "\n",
      "7. Conduct exploratory data analysis on numerical features, examining distributions and correlations with the target variable (SalePrice) to inform feature engineering decisions.\n",
      "\n",
      "8. Explore categorical features through frequency counts and box plots of SalePrice by category to understand their impact on the target variable.\n",
      "\n",
      "9. Engineer temporal features by calculating house age (current year - YearBuilt) and years since remodeling (current year - YearRemodAdd).\n",
      "\n",
      "10. Create a total square footage feature by combining relevant area measurements (basement, first floor, second floor) based on correlation analysis with the target.\n",
      "\n",
      "11. Calculate a weighted total bathroom count by adding full bathrooms and applying a 0.5 weight to half bathrooms, reflecting their relative value.\n",
      "\n",
      "12. Generate a binary feature indicating whether a property has a pool based on the Pool Area feature, potentially simplifying the relationship with price.\n",
      "\n",
      "13. Validate each engineered feature by examining its relationship with SalePrice and remove original features used in the engineering process to avoid redundancy.\n",
      "\n",
      "14. Check the distribution of the target variable (SalePrice) and consider transformations if significant skew is detected.\n",
      "\n",
      "15. Split the dataset into training (80%) and test (20%) sets using a fixed random seed for reproducibility.\n",
      "\n",
      "16. Create separate preprocessing pipelines for numerical features (median imputation followed by standard scaling) and categorical features (constant 'None' imputation followed by one-hot encoding).\n",
      "\n",
      "17. Combine the numerical and categorical preprocessing steps using a ColumnTransformer to create a unified preprocessing pipeline.\n",
      "\n",
      "18. Test multiple model types (Linear Regression, Random Forest, Gradient Boosting) on a small subset to identify promising candidates.\n",
      "\n",
      "19. Perform cross-validation with the Random Forest model, testing different hyperparameter values to optimize performance.\n",
      "\n",
      "20. Train the final Random Forest Regressor with optimal hyperparameters (n_estimators=150, max_depth=30, min_samples_split=5) using the complete preprocessing pipeline.\n",
      "\n",
      "21. Generate predictions on the test set and calculate multiple evaluation metrics (RMSE, MAE, RÂ²) to assess model performance from different perspectives.\n",
      "\n",
      "22. Create visualizations comparing predicted vs. actual home prices to identify patterns in prediction errors and potential model improvements.\n",
      "--------------------------------------------------\n",
      "Saved reference insights to: benchmark_final_test/storage/aarthi93-end-to-end-ml-pipeline/instructions/shards.md\n",
      "Processing: benchmark_final_test/storage/abdallaellaithy-titanic-in-space-ml-survival-predictions/instructions/gatekeeper_reference.md\n",
      "Project: abdallaellaithy-titanic-in-space-ml-survival-predictions\n",
      "Response: # Detailed Data Science Pipeline for Spaceship Titanic Classification\n",
      "\n",
      "1. Import essential data manipulation libraries (pandas, numpy), visualization libraries (matplotlib, seaborn), and basic machine learning tools (scikit-learn) to begin exploratory analysis.\n",
      "\n",
      "2. Import specialized modeling libraries (XGBoost, LightGBM) for gradient boosting implementation, ensuring all dependencies are properly installed.\n",
      "\n",
      "3. Load the training and test datasets from their specified paths, examining the returned shapes to verify successful data loading.\n",
      "\n",
      "4. Perform initial exploratory data analysis (EDA) to understand the structure of the data - check column types, basic statistics, and the distribution of the target variable 'Transported'.\n",
      "\n",
      "5. Investigate missing values across all features in both datasets to determine appropriate handling strategies for each column.\n",
      "\n",
      "6. Examine the 'Cabin' column structure to understand its components before attempting any feature extraction.\n",
      "\n",
      "7. Parse the 'Cabin' column to extract meaningful components: Deck (letter), Number (numeric value), and Side (P/S indicator) based on the observed pattern.\n",
      "\n",
      "8. Analyze the 'PassengerId' format to understand how passenger grouping information is encoded within this identifier.\n",
      "\n",
      "9. Extract group information from 'PassengerId' using string manipulation, creating a new feature to identify passengers traveling together.\n",
      "\n",
      "10. Create an 'IsAlone' binary flag to indicate passengers traveling without companions, which could be a meaningful predictor for survival.\n",
      "\n",
      "11. Investigate spending patterns by analyzing expense-related columns (RoomService, FoodCourt, ShoppingMall, Spa, VRDeck).\n",
      "\n",
      "12. Engineer a 'TotalSpent' feature by summing all spending categories and create a binary 'HasSpent' indicator for any non-zero spending.\n",
      "\n",
      "13. Fill missing spending values with zeros, assuming that missing values indicate no spending in that category.\n",
      "\n",
      "14. Combine train and test datasets to ensure consistent preprocessing, then apply all feature engineering steps to the combined data.\n",
      "\n",
      "15. Split the processed combined data back into training and test sets for model development.\n",
      "\n",
      "16. Analyze feature correlations and distributions to better understand relationships within the data and with the target variable.\n",
      "\n",
      "17. Create separate preprocessing pipelines for numerical and categorical features, testing different imputation strategies for each type.\n",
      "\n",
      "18. For numerical features, implement a pipeline with median imputation followed by standardization to handle missing values and scale features.\n",
      "\n",
      "19. For categorical features, test multiple approaches: most frequent imputation with one-hot encoding versus creating missing value categories.\n",
      "\n",
      "20. Build a composite preprocessing pipeline using ColumnTransformer to apply the appropriate transformations to each feature type.\n",
      "\n",
      "21. Define the feature set by dropping redundant or unnecessary columns, and establish the target variable for supervised learning.\n",
      "\n",
      "22. Split the training data into training and validation sets using stratification to maintain class balance across splits.\n",
      "\n",
      "23. Create baseline models using multiple algorithms (Random Forest, Gradient Boosting, XGBoost, LightGBM) with default parameters to establish performance benchmarks.\n",
      "\n",
      "24. Define an evaluation function that calculates and reports relevant metrics (accuracy and ROC AUC) for classification model comparison.\n",
      "\n",
      "25. Evaluate all baseline models on the validation set, storing results in a structured format for comparison.\n",
      "\n",
      "26. Select the best-performing model based on ROC AUC scores from the baseline evaluation.\n",
      "\n",
      "27. Define model-specific hyperparameter grids focusing on key parameters like tree depth, learning rate, and number of estimators.\n",
      "\n",
      "28. Implement hyperparameter tuning using GridSearchCV with 5-fold cross-validation on the best model to optimize performance.\n",
      "\n",
      "29. Extract and analyze the best hyperparameter configuration and corresponding performance metrics from the grid search results.\n",
      "\n",
      "30. Evaluate the tuned model on the validation set to measure performance improvement over the baseline model.\n",
      "\n",
      "31. Train the final model with optimized hyperparameters on the full training dataset to maximize learning from available data.\n",
      "\n",
      "32. Generate predictions on the test dataset using the final tuned model, formatting results according to submission requirements.\n",
      "\n",
      "33. Create a submission dataframe with PassengerId and predicted Transported values, ensuring proper boolean type conversion.\n",
      "\n",
      "34. Save the submission file in the required format for competition evaluation.\n",
      "--------------------------------------------------\n",
      "Saved reference insights to: benchmark_final_test/storage/abdallaellaithy-titanic-in-space-ml-survival-predictions/instructions/shards.md\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "# Find all instruction.md files in the storage directory\n",
    "storage_dir = \"benchmark_final_test/storage\"\n",
    "instruction_files = []\n",
    "\n",
    "# Walk through all directories under storage\n",
    "for root, dirs, files in os.walk(storage_dir):\n",
    "    # Check if this is an 'instructions' directory\n",
    "    if os.path.basename(root) == \"instructions\":\n",
    "        # Look for instructions.md file\n",
    "        instruction_file = os.path.join(root, \"gatekeeper_reference.md\")\n",
    "        if os.path.exists(instruction_file):\n",
    "            instruction_files.append(instruction_file)\n",
    "\n",
    "print(f\"Found {len(instruction_files)} instruction files\")\n",
    "\n",
    "# Process each instruction file\n",
    "for instruction_file in instruction_files:\n",
    "    print(f\"Processing: {instruction_file}\")\n",
    "    \n",
    "    # Load the instruction content\n",
    "    with open(instruction_file, \"r\") as f:\n",
    "        instruction_content = f.read()\n",
    "    \n",
    "    # Process the instruction content\n",
    "    modified_content = instruction_content\n",
    "    \n",
    "    # If the first line contains \"submission.csv\", remove it and the following newline\n",
    "    if \"submission.csv\" in modified_content.split('\\n')[0]:\n",
    "        modified_content = '\\n'.join(modified_content.split('\\n')[1:])\n",
    "    \n",
    "    # Replace double newlines with single newlines\n",
    "    modified_content = modified_content.replace('\\n\\n', '\\n')\n",
    "    \n",
    "    # Call the retriever with the modified instruction content\n",
    "    response = retriever.call_llm(modified_content, thinking={\"type\": \"enabled\", \"budget_tokens\": 8196})\n",
    "    # Print the project name and response\n",
    "    project_name = Path(instruction_file).parts[-3]  # Get the project name from the path\n",
    "    print(f\"Project: {project_name}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Save the response to reference_insights file in the same directory as instructions.md\n",
    "    shards_path = os.path.join(os.path.dirname(instruction_file), \"shards.md\")\n",
    "    with open(shards_path, \"w\") as f:\n",
    "        # If the response starts with #, remove the first line\n",
    "        if response.startswith(\"#\"):\n",
    "            response = \"\\n\".join(response.split(\"\\n\")[1:])\n",
    "        f.write(response)\n",
    "    print(f\"Saved reference insights to: {shards_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 12:27:11 - DataSciBench - INFO - Loaded configuration from llm_configs/raw_tianyu/llm_config_shards.toml\n",
      "2025-05-14 12:27:11 - DataSciBench - INFO - Initialized Gatekeeper with model: litellm_proxy/claude-3-7-sonnet, temperature: 1\n",
      "Found 21 instruction files\n",
      "Extracted knowledge from 21 projects\n",
      "Processing: benchmark_final/storage/ugurcan95-brazilian-tweet-sentiment-analysis/instructions/cleaned_instructions.md and benchmark_final/storage/ugurcan95-brazilian-tweet-sentiment-analysis/instructions/instructions.md\n",
      "**Your Knowledge**\n",
      "- The code uses Portuguese stopwords rather than English ones, indicating the analysis is specifically tailored for Brazilian Portuguese tweets, which requires language-specific NLP resources.\n",
      "\n",
      "- The text cleaning function removes URLs, hashtags, and mentions, suggesting these elements don't contribute meaningful sentiment information and might introduce noise in the analysis.\n",
      "\n",
      "- The implementation uses lemmatization rather than stemming, which preserves the semantic meaning of words by reducing them to their dictionary form based on part-of-speech, providing more accurate representations for sentiment analysis.\n",
      "\n",
      "- The TF-IDF vectorizer is configured with ngram_range=(1, 2), capturing both individual words and two-word phrases, which helps preserve contextual information that may be important for sentiment detection.\n",
      "\n",
      "- Logistic Regression is chosen as the classification model, likely because it works well with high-dimensional sparse text data and provides probabilistic outputs suitable for sentiment classification.\n",
      "\n",
      "- The model is configured with max_iter=500 (higher than default) to ensure convergence when training with the high-dimensional TF-IDF features, and n_jobs=-1 to utilize all available CPU cores for faster training.\n",
      "\n",
      "- The preprocessing pipeline (cleaning, stopword removal, lemmatization) is applied identically to both training and test data, ensuring consistency in feature representation across datasets.\n",
      "\n",
      "- The code doesn't implement any advanced sentiment-specific features like emoticon analysis or sentiment lexicons, suggesting this is a baseline approach that relies primarily on the statistical patterns of words.\n",
      "    \n",
      "Here are the original instructions:\n",
      "- Define a text cleaning function that performs multiple operations: converting to lowercase, removing URLs, retweet indicators, hashtags, mentions, punctuation, digits, special characters, HTML entities, and emojis. Apply this function to the tweet_text column.\n",
      "\n",
      "- Create and apply a function to remove Portuguese stopwords from each tweet, keeping only meaningful words in the text.\n",
      "\n",
      "- Implement lemmatization functions that reduce words to their base forms based on part-of-speech tags. First, define a helper function to lemmatize individual words based on their POS tags, then create a document-level function that tokenizes sentences, removes punctuation, tags words, and applies lemmatization. Apply this to the tweet_text column.\n",
      "\n",
      "- Prepare the training data by converting the cleaned text to TF-IDF features using a vectorizer that considers both unigrams and bigrams. Extract the target variable (sentiment) from the dataframe.\n",
      "\n",
      "- Train a Logistic Regression model with increased max iterations and parallel processing, and evaluate the model using accuracy and classification report metrics.\n",
      "\n",
      "- Prepare the test dataset by applying the same preprocessing steps as the training data: cleaning text, removing stopwords, and lemmatizing. Transform the preprocessed text using the same TF-IDF vectorizer fitted on the training data, then generate predictions using the trained model.\n",
      "    \n",
      "Project: ugurcan95-brazilian-tweet-sentiment-analysis\n",
      "Response: # Expanded Data Science Pipeline for Portuguese Tweet Sentiment Analysis\n",
      "\n",
      "1. [Data Exploration] Examine a sample of Portuguese tweets to understand typical language patterns, slang, abbreviations, and Twitter-specific elements that might affect sentiment analysis.\n",
      "\n",
      "2. [Text Cleaning] Create a comprehensive cleaning function that converts text to lowercase and removes URLs, retweet indicators, hashtags, mentions, punctuation, digits, special characters, HTML entities, and emojis.\n",
      "\n",
      "3. [Text Cleaning] Apply the cleaning function to the tweet_text column and verify results on a sample to ensure proper transformation without losing essential sentiment information.\n",
      "\n",
      "4. [Text Cleaning] Experiment with selectively preserving certain elements (like hashtags) in a separate analysis to determine if they contain valuable sentiment signals that might be lost in cleaning.\n",
      "\n",
      "5. [Text Normalization] Obtain Portuguese-specific stopword lists from NLP libraries rather than using English defaults, as language-specific resources are essential for accurate processing.\n",
      "\n",
      "6. [Text Normalization] Create and apply a function to remove Portuguese stopwords from each cleaned tweet, focusing on maintaining words that contribute to sentiment expression.\n",
      "\n",
      "7. [Text Normalization] Compare Portuguese stemming vs. lemmatization approaches using a sample of tweets to evaluate their effect on sentiment-bearing words and contextual meaning.\n",
      "\n",
      "8. [Text Normalization] Implement a POS-aware lemmatization function that first tokenizes text, tags parts of speech, then applies appropriate lemmatization rules specific to Portuguese words.\n",
      "\n",
      "9. [Text Normalization] Apply the lemmatization function to the cleaned tweets to reduce words to their base forms while preserving semantic meaning based on grammatical context.\n",
      "\n",
      "10. [Feature Engineering] Experiment with different text vectorization approaches (Count Vectorizer vs. TF-IDF) with various parameter settings to determine the optimal representation.\n",
      "\n",
      "11. [Feature Engineering] Configure the TF-IDF vectorizer with ngram_range=(1, 2) to capture both individual words and two-word phrases that provide contextual information for sentiment detection.\n",
      "\n",
      "12. [Model Selection] Split the dataset into training and validation sets using stratified sampling to maintain the distribution of sentiment classes.\n",
      "\n",
      "13. [Model Selection] Train and compare multiple classification algorithms (Naive Bayes, SVM) alongside Logistic Regression to establish performance benchmarks.\n",
      "\n",
      "14. [Model Selection] Configure Logistic Regression with max_iter=500 to ensure convergence with high-dimensional text features and n_jobs=-1 to utilize all available CPU cores.\n",
      "\n",
      "15. [Model Evaluation] Implement cross-validation and evaluate models using comprehensive metrics (accuracy, precision, recall, F1-score) to account for potential class imbalance.\n",
      "\n",
      "16. [Model Evaluation] Analyze misclassified Portuguese tweets to identify challenging language patterns or expressions that might require specialized handling.\n",
      "\n",
      "17. [Pipeline Implementation] Create an end-to-end pipeline that integrates all text preprocessing steps (cleaning, stopword removal, lemmatization) with TF-IDF vectorization and model training.\n",
      "\n",
      "18. [Test Data Processing] Apply identical preprocessing steps to the test dataset: cleaning text, removing Portuguese stopwords, and lemmatizing with the same parameters used for training data.\n",
      "\n",
      "19. [Test Data Processing] Transform the preprocessed test text using the TF-IDF vectorizer fitted on training data to ensure feature consistency between training and prediction.\n",
      "\n",
      "20. [Prediction Generation] Generate sentiment predictions on the processed test data using the trained Logistic Regression model and evaluate performance with appropriate metrics.\n",
      "--------------------------------------------------\n",
      "Saved shards to: benchmark_final/storage/ugurcan95-brazilian-tweet-sentiment-analysis/instructions/shards.md\n",
      "Processing: benchmark_final/storage/mightyjiraiya-titanic-survival-prediction/instructions/cleaned_instructions.md and benchmark_final/storage/mightyjiraiya-titanic-survival-prediction/instructions/instructions.md\n",
      "**Your Knowledge**\n",
      "- The code extracts titles from passenger names and groups them strategically, recognizing that titles can be a proxy for social status and age, which were significant factors in survival priority during the Titanic disaster.\n",
      "\n",
      "- Family size is engineered as a feature and further categorized into \"Alone,\" \"Small,\" and \"Large\" groups, reflecting the understanding that family dynamics affected survival chances differently (small families might have stayed together and helped each other, while very large families might have been separated).\n",
      "\n",
      "- The presence of cabin information (HasCabin) is used as a feature, suggesting that having a cabin record is a proxy for passenger class and organization, which could indicate priority during evacuation.\n",
      "\n",
      "- Deck information is extracted from cabin numbers, recognizing the ship's architecture where different decks had varying proximity to lifeboats and exits, directly impacting survival chances.\n",
      "\n",
      "- Age imputation is performed using a sophisticated approach based on Title, Sex, and Pclass groups, rather than a simple mean/median, acknowledging the strong correlations between these variables and age in the Titanic's social context.\n",
      "\n",
      "- The interaction feature 'Age*Class' is created to capture the combined effect of age and passenger class, reflecting the \"women and children first\" policy that was applied differently across passenger classes.\n",
      "\n",
      "- 'FarePerPerson' is calculated to normalize the fare by family size, recognizing that the raw fare might not accurately represent a passenger's economic status if they purchased tickets for multiple family members.\n",
      "\n",
      "- The code uses StratifiedKFold for cross-validation to maintain the same proportion of survival outcomes in each fold, addressing the class imbalance issue in the Titanic dataset.\n",
      "\n",
      "- The final model is an ensemble of the top three performing models using soft voting, leveraging the strengths of different algorithms while mitigating their individual weaknesses to improve prediction robustness.\n",
      "\n",
      "- The preprocessing pipeline handles numeric and categorical features differently, applying appropriate transformations (scaling for numeric features, one-hot encoding for categorical features) to optimize model performance.\n",
      "    \n",
      "Here are the original instructions:\n",
      "- Create a comprehensive feature engineering function that:\n",
      "1. Extracts titles from passenger names and groups rare titles\n",
      "2. Creates family-related features (family size, is alone flag, family group categories)\n",
      "3. Processes cabin information (has cabin flag, deck extraction)\n",
      "4. Handles ticket information (prefix extraction, numeric conversion)\n",
      "5. Bins age and fare into categorical groups\n",
      "6. Creates interaction features (Age*Class, Fare per person)\n",
      "Then apply this function to both training and test datasets.\n",
      "\n",
      "- Handle missing values in the datasets by:\n",
      "1. Filling missing Age values based on Title, Sex, and Pclass group medians\n",
      "2. Imputing missing Embarked values with the mode\n",
      "3. Filling missing Fare values in the test set with median fares by Pclass\n",
      "4. Recalculating age and fare bins after imputation to ensure consistency\n",
      "\n",
      "- Prepare the data for modeling by:\n",
      "1. Selecting relevant features for prediction\n",
      "2. Separating features into numeric and categorical groups\n",
      "3. Creating preprocessing pipelines with appropriate imputation and scaling/encoding\n",
      "4. Setting up a ColumnTransformer to apply the right transformations to each feature type\n",
      "\n",
      "- Build and evaluate multiple machine learning models by:\n",
      "1. Creating a dictionary of models (Logistic Regression, Random Forest, Gradient Boosting, SVC)\n",
      "2. Setting up pipelines that combine preprocessing with each classifier\n",
      "3. Defining a cross-validation strategy using StratifiedKFold\n",
      "4. Evaluating each model using cross-validation and storing the results\n",
      "\n",
      "- Perform hyperparameter tuning for the top-performing models by:\n",
      "1. Defining parameter grids for each model type\n",
      "2. Using GridSearchCV with cross-validation to find optimal parameters\n",
      "3. Selecting the best models based on cross-validation performance\n",
      "\n",
      "- Create an ensemble model by:\n",
      "1. Combining the best-tuned models into a VotingClassifier with soft voting\n",
      "2. Training the ensemble on the full training dataset\n",
      "3. Evaluating the ensemble using cross-validation\n",
      "4. Comparing the ensemble performance with individual model performance\n",
      "\n",
      "- Generate predictions and create a submission file by:\n",
      "1. Using the trained ensemble model to predict survival on the test dataset\n",
      "2. Creating a submission dataframe with PassengerId and predicted Survived values\n",
      "3. Saving the predictions to a CSV file for submission\n",
      "    \n",
      "Project: mightyjiraiya-titanic-survival-prediction\n",
      "Response: 1. [Exploratory analysis] Conduct initial exploratory data analysis (EDA) focusing on the relationship between survival and passenger characteristics (sex, age, class), noting the \"women and children first\" policy's effect across different passenger classes.\n",
      "\n",
      "2. [Feature engineering] Extract titles from passenger names (Mr, Mrs, Miss, Master, etc.) using regex patterns and analyze title distribution across survival outcomes, as titles reflect social status that influenced evacuation priority.\n",
      "\n",
      "3. [Feature engineering] Group rare titles into meaningful categories (e.g., military, nobility, religious) based on their distribution and survival rates to create a more manageable 'Title' feature.\n",
      "\n",
      "4. [Feature engineering] Create family-related features including 'FamilySize' (SibSp + Parch + 1) and 'IsAlone' flag, then categorize into 'Alone', 'Small' (2-4), and 'Large' (5+) based on observed survival patterns.\n",
      "\n",
      "5. [Feature engineering] Create a 'HasCabin' binary feature from cabin information, recognizing that cabin record presence indicates passenger class and organization during evacuation.\n",
      "\n",
      "6. [Feature engineering] Extract deck information (A-G) from cabin numbers, as different decks had varying proximity to lifeboats and exits, directly impacting survival chances.\n",
      "\n",
      "7. [Feature engineering] Process ticket information by extracting prefixes and numeric portions, evaluating whether this improves model performance compared to using just passenger class.\n",
      "\n",
      "8. [Feature engineering] Bin Age and Fare into categorical groups based on observed survival rate patterns rather than arbitrary boundaries to better capture non-linear relationships with survival.\n",
      "\n",
      "9. [Feature engineering] Create interaction features including 'Age*Class' to capture how age-based priority varied by class, and 'FarePerPerson' (Fare divided by FamilySize) to normalize economic status indicators.\n",
      "\n",
      "10. [Missing values] For Age imputation, implement a group-based approach using Title, Sex, and Pclass group medians, which captures demographic patterns more accurately than simple mean/median imputation.\n",
      "\n",
      "11. [Missing values] Impute the few missing Embarked values with the mode and any missing Fare values with Pclass-based medians to maintain socioeconomic patterns in the data.\n",
      "\n",
      "12. [Missing values] Recalculate all binned and derived features after imputation to ensure consistency between original and imputed values.\n",
      "\n",
      "13. [Feature selection] Select features most predictive of survival based on statistical tests and domain knowledge about the Titanic disaster, focusing on gender, age, class, and family status.\n",
      "\n",
      "14. [Preprocessing] Separate features into numeric (Age, Fare, family size) and categorical (Sex, Pclass, Title, Deck) groups for appropriate transformation.\n",
      "\n",
      "15. [Preprocessing] Create preprocessing pipelines for numeric features (imputation, scaling) and categorical features (imputation, encoding) to ensure consistent transformation.\n",
      "\n",
      "16. [Preprocessing] Set up a ColumnTransformer to apply the right transformations to each feature type, ensuring a consistent preprocessing approach for training and test data.\n",
      "\n",
      "17. [Model selection] Create a dictionary of models including Logistic Regression, Random Forest, Gradient Boosting, and SVC, which collectively cover linear, tree-based, and kernel approaches to classification.\n",
      "\n",
      "18. [Model validation] Set up StratifiedKFold cross-validation to maintain the same proportion of survival outcomes in each fold, addressing the class imbalance in the Titanic dataset.\n",
      "\n",
      "19. [Model validation] Evaluate models using multiple metrics (accuracy, precision, recall, F1-score, ROC-AUC) with emphasis on those most relevant to the competition scoring.\n",
      "\n",
      "20. [Hyperparameter tuning] For top-performing models, define parameter grids covering key hyperparameters that significantly impact model performance.\n",
      "\n",
      "21. [Hyperparameter tuning] Use GridSearchCV with cross-validation to find optimal parameters for each model type, balancing model complexity against performance.\n",
      "\n",
      "22. [Ensemble creation] Combine the best-tuned models into a VotingClassifier with soft voting, leveraging the strengths of different algorithms while mitigating their individual weaknesses.\n",
      "\n",
      "23. [Ensemble validation] Evaluate the ensemble using cross-validation and compare its performance with individual model performance to determine if it provides meaningful improvement.\n",
      "\n",
      "24. [Final prediction] Train the final ensemble on the full training dataset and generate survival predictions for the test set, checking for reasonable distribution of predictions.\n",
      "\n",
      "25. [Submission preparation] Create a submission dataframe with PassengerId and predicted Survived values, saving to CSV for competition submission.\n",
      "--------------------------------------------------\n",
      "Saved shards to: benchmark_final/storage/mightyjiraiya-titanic-survival-prediction/instructions/shards.md\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "from copy import deepcopy\n",
    "from llms.llm_interact import LLMConfig\n",
    "from llms.retriever import Retriever\n",
    "from llm_interact_env import Environment, EnvironmentConfig, Task, run\n",
    "from logger import logger, configure_global_logger\n",
    "import subprocess\n",
    "\n",
    "retriever_config = LLMConfig.from_toml(\"llm_configs/raw_tianyu/llm_config_shards.toml\")\n",
    "retriever = Retriever(retriever_config)\n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Find all instruction.md files in the storage directory\n",
    "storage_dir = \"benchmark_final/storage\"\n",
    "instruction_files = []\n",
    "knowledge_files = []\n",
    "\n",
    "# Walk through all directories under storage\n",
    "for root, dirs, files in os.walk(storage_dir):\n",
    "    # Check if this is an 'instructions' directory\n",
    "    if os.path.basename(root) == \"instructions\":\n",
    "        # Look for instructions.md and cleaned_instructions.md files\n",
    "        try:\n",
    "            cleaned_instruction_file = os.path.join(root, \"cleaned_instructions.md\")\n",
    "            if os.path.exists(cleaned_instruction_file):\n",
    "                instruction_files.append(cleaned_instruction_file)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"cleaned_instructions.md not found in {root}\")\n",
    "            if os.path.exists(os.path.join(root, \"instructions.md\")):\n",
    "                knowledge_files.append(os.path.join(root, \"instructions.md\"))\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"instructions.md not found in {root}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {root}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Found {len(instruction_files)} instruction files\")\n",
    "print(f\"Extracted knowledge from {len(knowledge_files)} projects\")\n",
    "\n",
    "# Process each instruction file\n",
    "for instruction_file, knowledge_file in zip(instruction_files[:2], knowledge_files[:2]):\n",
    "    print(f\"Processing: {instruction_file} and {knowledge_file}\")\n",
    "    \n",
    "    # Load the cleaned instruction content\n",
    "    with open(instruction_file, \"r\") as f:\n",
    "        instruction_content = f.read()\n",
    "\n",
    "    with open(knowledge_file, \"r\") as f:\n",
    "        original_content = f.read()\n",
    "    # Extract content after \"**Your Knowledge**\"\n",
    "    if \"**Your Knowledge**\" in original_content:\n",
    "        knowledge_start_idx = original_content.find(\"**Your Knowledge**\")\n",
    "        if knowledge_start_idx != -1:\n",
    "            # Get content after \"**Your Knowledge**\" header\n",
    "            extracted_knowledge = original_content[knowledge_start_idx:]\n",
    "    \n",
    "    # Get the project name\n",
    "    project_name = Path(instruction_file).parts[-3]  # Get the project name from the path\n",
    "    \n",
    "    print(extracted_knowledge + \"\\n\" + \"Here are the original instructions:\\n\" + instruction_content)\n",
    "    # Call the retriever with the instruction content\n",
    "    response = retriever.call_llm(extracted_knowledge + \"\\n\" + \"Here are the original instructions\" + instruction_content, thinking={\"type\": \"enabled\", \"budget_tokens\": 20000})\n",
    "    \n",
    "    # Print the project name and response\n",
    "    print(f\"Project: {project_name}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Save the response to shards file in the same directory as instructions.md\n",
    "    shards_path = os.path.join(os.path.dirname(instruction_file), \"shards.md\")\n",
    "    with open(shards_path, \"w\") as f:\n",
    "        # If the response starts with #, remove the first line\n",
    "        if response.startswith(\"#\"):\n",
    "            response = \"\\n\".join(response.split(\"\\n\")[1:])\n",
    "        f.write(response)\n",
    "    print(f\"Saved shards to: {shards_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ugurcan95-brazilian-tweet-sentiment-analysis': \"**Your Knowledge**\\n- The code uses Portuguese stopwords rather than English ones, indicating the analysis is specifically tailored for Brazilian Portuguese tweets, which requires language-specific NLP resources.\\n\\n- The text cleaning function removes URLs, hashtags, and mentions, suggesting these elements don't contribute meaningful sentiment information and might introduce noise in the analysis.\\n\\n- The implementation uses lemmatization rather than stemming, which preserves the semantic meaning of words by reducing them to their dictionary form based on part-of-speech, providing more accurate representations for sentiment analysis.\\n\\n- The TF-IDF vectorizer is configured with ngram_range=(1, 2), capturing both individual words and two-word phrases, which helps preserve contextual information that may be important for sentiment detection.\\n\\n- Logistic Regression is chosen as the classification model, likely because it works well with high-dimensional sparse text data and provides probabilistic outputs suitable for sentiment classification.\\n\\n- The model is configured with max_iter=500 (higher than default) to ensure convergence when training with the high-dimensional TF-IDF features, and n_jobs=-1 to utilize all available CPU cores for faster training.\\n\\n- The preprocessing pipeline (cleaning, stopword removal, lemmatization) is applied identically to both training and test data, ensuring consistency in feature representation across datasets.\\n\\n- The code doesn't implement any advanced sentiment-specific features like emoticon analysis or sentiment lexicons, suggesting this is a baseline approach that relies primarily on the statistical patterns of words.\\n    \",\n",
       " 'mightyjiraiya-titanic-survival-prediction': '**Your Knowledge**\\n- The code extracts titles from passenger names and groups them strategically, recognizing that titles can be a proxy for social status and age, which were significant factors in survival priority during the Titanic disaster.\\n\\n- Family size is engineered as a feature and further categorized into \"Alone,\" \"Small,\" and \"Large\" groups, reflecting the understanding that family dynamics affected survival chances differently (small families might have stayed together and helped each other, while very large families might have been separated).\\n\\n- The presence of cabin information (HasCabin) is used as a feature, suggesting that having a cabin record is a proxy for passenger class and organization, which could indicate priority during evacuation.\\n\\n- Deck information is extracted from cabin numbers, recognizing the ship\\'s architecture where different decks had varying proximity to lifeboats and exits, directly impacting survival chances.\\n\\n- Age imputation is performed using a sophisticated approach based on Title, Sex, and Pclass groups, rather than a simple mean/median, acknowledging the strong correlations between these variables and age in the Titanic\\'s social context.\\n\\n- The interaction feature \\'Age*Class\\' is created to capture the combined effect of age and passenger class, reflecting the \"women and children first\" policy that was applied differently across passenger classes.\\n\\n- \\'FarePerPerson\\' is calculated to normalize the fare by family size, recognizing that the raw fare might not accurately represent a passenger\\'s economic status if they purchased tickets for multiple family members.\\n\\n- The code uses StratifiedKFold for cross-validation to maintain the same proportion of survival outcomes in each fold, addressing the class imbalance issue in the Titanic dataset.\\n\\n- The final model is an ensemble of the top three performing models using soft voting, leveraging the strengths of different algorithms while mitigating their individual weaknesses to improve prediction robustness.\\n\\n- The preprocessing pipeline handles numeric and categorical features differently, applying appropriate transformations (scaling for numeric features, one-hot encoding for categorical features) to optimize model performance.\\n    ',\n",
       " 'sasakitetsuya-predicting-startup-valuation-with-machine-learning': '**Your Knowledge**\\n- The log transformation of monetary values (Investment Amount, Valuation) and derived ratios indicates the data likely has a right-skewed distribution, which is common in financial datasets where values can span several orders of magnitude.\\n\\n- The creation of relative metrics (Growth Rate Relative to Industry, Investment Relative to Country) demonstrates an understanding that startup performance should be evaluated within the context of industry and geographic norms rather than absolute values.\\n\\n- The \"Funding Efficiency Ratio\" metric suggests that efficiently converting investor participation into funding is considered an important indicator of startup success or attractiveness.\\n\\n- The inclusion of \"Startup Age\" as a feature implies that the maturity of a company is considered relevant to its valuation, recognizing the different growth stages startups go through.\\n\\n- The \"Early Stage\" binary feature indicates that there\\'s a meaningful distinction between early-stage and later-stage startups in terms of their valuation dynamics.\\n\\n- The choice of RandomForestRegressor suggests the relationship between features and startup valuation is likely non-linear and potentially involves complex interactions between variables.\\n\\n- The model predicts the log-transformed valuation rather than the raw value, indicating an understanding that prediction errors should be proportional to the valuation magnitude rather than absolute.\\n\\n- The creation of \"Funding Rounds Per Year\" normalizes for different company lifespans, recognizing that the pace of fundraising is more informative than the absolute number of rounds.\\n\\n- The \"Investor Density\" metric suggests that having more investors per funding round might signal different investment dynamics or company attractiveness.\\n    ',\n",
       " 'jakubkrasuski-llm-chatbot-arena-predicting-user-preferences': \"**Your Knowledge**\\n- The approach uses the difference between TF-IDF vectors as features rather than concatenating them, which implicitly assumes that the relative difference in word usage between responses is more important than their absolute content for predicting user preferences.\\n\\n- The code handles JSON parsing errors gracefully by returning the original string if parsing fails, suggesting that the data might contain inconsistently formatted entries that need to be handled robustly.\\n\\n- When creating the target variable, the code includes a fallback option (defaulting to label 2/tie) for rows that don't have any winner marked, indicating an assumption that ambiguous cases should be treated as ties rather than being excluded.\\n\\n- The TF-IDF vectorizer is fitted on both training and test data together, which prevents vocabulary mismatch issues but assumes that the test set is available during model development and that there won't be new, unseen terms in future data.\\n\\n- The choice of multinomial logistic regression with increased max_iter suggests that the developer anticipated potential convergence issues, which often occur with high-dimensional sparse text features.\\n\\n- The model evaluation uses stratified k-fold cross-validation with log loss, indicating that maintaining class distribution across folds is important and that the quality of probability estimates (not just class predictions) matters for this application.\\n\\n- The feature engineering approach (combining prompt with each response) implicitly assumes that the context of the conversation is crucial for evaluating response quality, rather than evaluating responses in isolation.\\n\\n- The code uses a maximum of 10,000 TF-IDF features with unigrams and bigrams, suggesting a balance between capturing enough linguistic patterns while avoiding overfitting on rare n-grams or creating an excessively high-dimensional feature space.\\n    \",\n",
       " 'vijaythurimella-bank-subscriptions-predictions-f1-score': '**Your Knowledge**\\n- The code uses SMOTE for oversampling the minority class rather than undersampling the majority class, suggesting that preserving information in the majority class is important while still addressing class imbalance.\\n\\n- The model uses a lower prediction threshold (0.4 instead of the default 0.5) to increase positive predictions, indicating that false negatives are likely more costly than false positives in this bank subscription context.\\n\\n- The code applies different preprocessing strategies for numerical and categorical features (median imputation for numerical features and mode imputation for categorical features), reflecting best practices for handling missing values based on data type.\\n\\n- Test data is imputed using statistics (median, mode) from the training data to prevent data leakage, which would occur if test data information influenced the imputation strategy.\\n\\n- The XGBoost hyperparameters are tuned to prevent overfitting (reduced max_depth, increased min_child_weight) while still capturing complex patterns in the data, suggesting the data may have complex but subtle relationships.\\n\\n- The model evaluation uses F1 score rather than accuracy, indicating that both precision and recall are important in this classification task, which is typical for imbalanced datasets where accuracy can be misleading.\\n\\n- The scale_pos_weight parameter is set to 3, suggesting that positive examples (customers subscribing to the bank product) are approximately 3 times less frequent than negative examples in the original dataset.\\n\\n- The code uses a relatively small learning rate (0.04) combined with a large number of estimators (1000), implementing the common XGBoost best practice of \"slow learning\" to improve generalization performance.\\n\\n- Cross-validation is performed on the original imbalanced dataset rather than the SMOTE-resampled data, which provides a more realistic estimate of model performance on new, imbalanced data.\\n\\n- The final model is trained on SMOTE-resampled data but evaluated and used for prediction on imbalanced data, showing an understanding that while training benefits from balanced classes, real-world application involves imbalanced data.\\n    ',\n",
       " 'shaswatatripathy-store-sales-prediction': \"**Your Knowledge**\\n- The code uses forward and backward fill methods for handling missing oil prices, suggesting that oil prices tend to remain relatively stable over short periods and that temporal continuity is more important than using statistical measures like mean or median.\\n\\n- Only non-transferred holidays are flagged as holidays in the dataset, indicating that the actual date of celebration is more relevant for sales prediction than the official holiday date when transferred.\\n\\n- The creation of lag features (7 and 14 days) suggests weekly seasonality in sales patterns, with the assumption that sales from the same day in previous weeks are strong predictors of current sales.\\n\\n- The 7-day rolling average is shifted by 1 day to avoid data leakage, demonstrating awareness that including current day sales in the rolling average would create an invalid feature that wouldn't be available at prediction time.\\n\\n- The code creates a promotion ratio feature (items on promotion divided by transactions), indicating that the effectiveness of promotions relative to store traffic is more informative than the raw count of promotional items.\\n\\n- The model uses RMSLE (Root Mean Squared Logarithmic Error) instead of RMSE, suggesting that the relative error is more important than absolute error, which is typical in sales forecasting where the percentage error matters more than the absolute difference.\\n\\n- The validation set is created using the last 90 days of training data, respecting the temporal nature of the data and simulating how the model would perform in a real-world forecasting scenario.\\n\\n- The code ensures predictions are non-negative by applying a maximum function, recognizing that negative sales values are impossible in this domain.\\n\\n- The model uses store number and product family as categorical features rather than numerical features, acknowledging that these are discrete entities without inherent numerical relationships.\\n\\n- The early stopping mechanism with a patience of 100 rounds balances between giving the model enough time to learn complex patterns and preventing overfitting on the training data.\\n    \",\n",
       " 'drpashamd4r-indian-floods-data-exploratory': '**Your Knowledge**\\n- The code assumes that missing values in impact columns (\\'Human fatality\\', \\'Human injured\\', \\'Human Displaced\\', \\'Animal Fatality\\') represent zero reported impacts rather than unknown values, which is a domain-specific assumption about how the data was collected.\\n\\n- When creating the classification target, the code uses the median of non-zero fatalities as the threshold for \"high impact\" events, suggesting that the distribution of fatalities is likely skewed with many zero values, which is common in disaster impact data.\\n\\n- The code includes multiple fallback mechanisms for determining the classification threshold, indicating an awareness that disaster datasets often contain sparse or imbalanced impact data that could result in problematic thresholds.\\n\\n- The feature selection focuses on temporal aspects (duration, season via month and day of year) and geographical location (State), reflecting domain knowledge that flood impacts are heavily influenced by when and where they occur.\\n\\n- The preprocessing strategy uses median imputation for numerical features rather than mean, suggesting an awareness that flood data likely contains outliers that would skew mean-based imputation.\\n\\n- The code floors negative predictions at zero, reflecting the domain constraint that fatality counts cannot be negative, even if the statistical model might produce such values.\\n\\n- The inclusion of \\'Start_DayOfYear\\' as a feature indicates an understanding that seasonal patterns are important for flood prediction, potentially capturing monsoon cycles or seasonal weather patterns in India.\\n\\n- The choice to use both Random Forest and LightGBM models suggests an understanding that tree-based ensemble methods often perform well on tabular data with mixed numerical and categorical features, especially for datasets of moderate size.\\n\\n- The code calculates flood duration as inclusive (adding 1 to the difference between end and start dates), reflecting domain knowledge about how flood duration is typically reported and measured.\\n    ',\n",
       " 'esotericdata1-titanickaggle-ds': '**Your Knowledge**\\n- Missing Age values are imputed based on passenger class (Pclass) medians rather than the overall median, reflecting the understanding that age distributions differ across passenger classes on the Titanic.\\n\\n- The first letter of the Cabin number (CabinLetter) is extracted as a feature because on the Titanic, cabin letters indicated the deck location, which could be related to survival chances based on proximity to lifeboats or exits.\\n\\n- Family size is calculated and used to create an \"IsAlone\" indicator, suggesting that traveling alone versus with family members affected survival probability on the Titanic.\\n\\n- The code retains SibSp (siblings/spouse) and Parch (parents/children) as individual features while also creating a combined FamilySize feature, indicating that both the specific family relationships and overall family size might have different effects on survival.\\n\\n- The model uses RandomForestClassifier, which is appropriate for this classification task as it handles non-linear relationships and interactions between features that were likely present in survival patterns on the Titanic.\\n\\n- The hyperparameter tuning focuses on tree complexity (max_depth), ensemble size (n_estimators), and leaf node criteria (min_samples_split, min_samples_leaf), suggesting these are the most important factors for model performance on this dataset.\\n\\n- The code drops Name, Ticket, and Cabin columns after extracting useful information (like CabinLetter), indicating that while the raw values aren\\'t directly useful for prediction, derived features from them can capture relevant patterns.\\n\\n- The preprocessing strategy handles missing values differently for different features (median imputation for Age and Fare, mode imputation for Embarked), reflecting domain knowledge about the nature of these missing values in the Titanic dataset.\\n    ',\n",
       " 'ayodejiibrahimlateef-integrative-analysis-early-depression-detection': \"**Your Knowledge**\\n- The code treats depression as a binary outcome (0 or 1), suggesting the dataset represents depression as a dichotomous state rather than a continuous spectrum of severity.\\n\\n- Sleep duration is considered a significant factor for mental health, with the code mapping textual descriptions to numeric values centered around healthy sleep ranges (7-8 hours mapped to 7.5).\\n\\n- The creation of 'Stress_Score' by averaging academic and work pressure implies these two stressors are considered equally important contributors to overall stress levels.\\n\\n- In the 'Lifestyle_Balance' metric, sleep quality is weighted more heavily (40%) than satisfaction metrics (30% each), suggesting sleep is considered the most important lifestyle factor affecting mental health.\\n\\n- The code includes family history of mental illness as a predictor, acknowledging the genetic and environmental familial components of depression risk.\\n\\n- The model includes both direct measurements (like 'Academic Pressure') and synthesized features (like 'Stress_Score'), indicating that derived metrics may capture important patterns not evident in individual variables alone.\\n\\n- Missing values are handled through imputation rather than row deletion, suggesting a preference for preserving data points even when incomplete.\\n\\n- The code uses StandardScaler for lifestyle features before combining them, acknowledging that these variables have different scales and distributions that need to be normalized before meaningful comparison.\\n\\n- The logistic regression model's maximum iterations is increased to 1000 (from the default 100), suggesting potential convergence issues with the default settings, possibly due to complex relationships in the data.\\n\\n- CGPA (Cumulative Grade Point Average) is included as a predictor, indicating academic performance is considered potentially relevant to depression risk in students.\\n    \",\n",
       " 'dmytrobuhai-eda-rf': \"**Your Knowledge**\\n- The code treats rainfall prediction as a binary classification problem rather than regression, suggesting that the target variable is likely binary (rain/no rain) rather than continuous rainfall amounts.\\n\\n- Wind direction is classified into three categories (moist, neutral, dry) based on meteorological knowledge that certain wind directions are associated with different moisture levels, which can be important predictors for rainfall.\\n\\n- The outlier handling approach preserves the multivariate relationships in the data by replacing outliers with the nearest valid values in feature space rather than simply removing or capping them.\\n\\n- The code drops 'maxtemp' and 'mintemp' features in the final model, suggesting that after experimentation, these temperature features were found to be less predictive of rainfall than other variables.\\n\\n- The model evaluation prioritizes ROC-AUC over accuracy, indicating that the ability to rank predictions correctly is more important than the exact classification threshold, which is common in weather prediction where probability estimates are valuable.\\n\\n- The hyperparameter tuning focuses on tree-based parameters that control model complexity (max_depth, min_samples_split) and ensemble diversity (n_estimators), which are critical factors in preventing overfitting in Random Forest models.\\n\\n- Missing values in the test set are handled by mean imputation, which is a simple but effective approach when the amount of missing data is limited and when more complex imputation methods might introduce bias.\\n\\n- The code uses stratification during train-test splitting, indicating awareness that the target variable may be imbalanced, which is common in rainfall prediction where rainy days might be less frequent than non-rainy days.\\n    \",\n",
       " 'patilaakash619-backpack-price-prediction-ml-guide': \"**Your Knowledge**\\n- The code handles missing values differently based on data type (median for numerical, mode for categorical), suggesting an understanding that extreme values could skew means, making medians more robust for numerical data.\\n\\n- The feature engineering includes creating a 'capacity_per_compartment' feature, indicating domain knowledge that the weight capacity relative to the number of compartments is likely a meaningful predictor of backpack price.\\n\\n- The code creates a 'price_range' categorical feature from the continuous 'Price' variable, suggesting that price categories might be more interpretable or useful for certain analyses than raw price values.\\n\\n- The model selection includes three tree-based ensemble methods (XGBoost, Random Forest, Gradient Boosting) but no linear models, suggesting an expectation that the relationship between features and backpack prices is likely non-linear.\\n\\n- The code evaluates models using multiple metrics (RMSE, RÂ², MAE) rather than optimizing for a single metric, indicating an understanding that different error measures provide complementary information about model performance.\\n\\n- The feature encoding approach uses one-hot encoding with drop_first=True, which indicates knowledge about avoiding the dummy variable trap that can cause multicollinearity issues in regression models.\\n\\n- The code standardizes features before model training, suggesting awareness that tree-based models can benefit from standardization when features have different scales, particularly for gradient-based optimizers in XGBoost and Gradient Boosting.\\n\\n- The code removes 'is_premium' from the feature set, suggesting domain knowledge that this might be a derived feature or one that would cause data leakage by being too closely related to the target variable (Price).\\n    \",\n",
       " 'patilaakash619-electric-vehicle-population-data-in-the-us': '**Your Knowledge**\\n- The code handles missing values differently based on data type: categorical variables are filled with \\'Unknown\\' to preserve their categorical nature, while numerical variables are filled with median values to minimize the impact of outliers, suggesting a thoughtful approach to maintaining data integrity.\\n\\n- The creation of the \\'Is_BEV\\' binary feature indicates that distinguishing between battery electric vehicles and other types (like plug-in hybrids) is important for the analysis, suggesting these categories have significantly different characteristics.\\n\\n- The price categorization (Budget, Mid-range, Premium, Luxury) with specific thresholds at $30,000, $60,000, and $100,000 reflects domain knowledge about meaningful price segments in the electric vehicle market.\\n\\n- The range categorization with thresholds at 50, 150, and 300 miles suggests these are meaningful breakpoints in electric vehicle range capabilities that likely correspond to different use cases or consumer segments.\\n\\n- The code assumes 2025 as the current year for calculating vehicle age, which indicates the dataset includes future model years or the analysis is forward-looking, accounting for vehicles that may not yet be on the road.\\n\\n- The simplification of model names to their base form (\\'Model_Base\\') suggests that model variants (like \"Model S Performance\") share common characteristics with their base models, and grouping them can provide more robust analysis.\\n\\n- The creation of the \\'Is_Popular_Make\\' feature indicates that vehicles from the top 5 manufacturers may have different characteristics or market behaviors compared to those from less common manufacturers.\\n\\n- The choice of Random Forest for the regression task suggests the relationship between features and electric range is likely non-linear and may involve complex interactions between variables that tree-based models can capture effectively.\\n\\n- The code uses a fixed random state (42) for data splitting, ensuring reproducibility of results across different runs of the analysis, which is important for scientific validity and debugging.\\n\\n- The selection of features for the regression model prioritizes vehicle specifications (Model Year, Make, Type) and economic factors (Base MSRP) rather than geographic information (County, City), suggesting these are more predictive of electric range.\\n    ',\n",
       " 'iseedeep-mission-podcast-listening-prediction': \"**Your Knowledge**\\n- The 'id' column is dropped because it's just an identifier and doesn't contain information that would help predict listening time, avoiding potential overfitting to meaningless patterns.\\n\\n- Missing numerical values are imputed with the median rather than the mean, suggesting a concern about outliers that could skew the mean values.\\n\\n- Categorical features like 'Podcast_Name' and 'Episode_Title' are encoded using LabelEncoder rather than one-hot encoding, indicating these features likely have many unique values which would create too many columns if one-hot encoded.\\n\\n- The code creates cyclical features for days of the week (sin and cos transformations) to capture the circular nature of weekly patterns, preserving the relationship between Sunday and Monday.\\n\\n- The feature 'popularity_diff' suggests that the relative difference between host and guest popularity might be more predictive than their absolute popularity values.\\n\\n- Sentiment is mapped to a numerical scale (-1, 0, 1) that preserves the ordinal relationship between negative, neutral, and positive sentiments.\\n\\n- Episode length is binned into categories before one-hot encoding, indicating that the relationship between episode length and listening time might be non-linear or categorical in nature.\\n\\n- The code uses 'drop_first=True' when one-hot encoding categorical variables to avoid the dummy variable trap, which would create multicollinearity in the model.\\n\\n- The train and test datasets are aligned with 'join=left', ensuring that any features present in the training data but not in the test data are added to the test data with zero values, maintaining model compatibility.\\n\\n- The hyperparameter tuning focuses on three key XGBoost parameters (n_estimators, max_depth, learning_rate) with relatively small search spaces, suggesting a balance between finding good parameters and computational efficiency.\\n    \",\n",
       " 'hasangulec-feature-engineering-diabetes': '**Your Knowledge**\\n- Zero values in medical measurements like Glucose, BloodPressure, SkinThickness, Insulin, and BMI are likely recording errors or missing data rather than actual zero measurements, which is why they\\'re treated as missing values and imputed.\\n\\n- The code uses the 5th and 95th percentiles (q1=0.05, q3=0.95) rather than the traditional 25th and 75th percentiles for outlier detection, suggesting a more conservative approach to outlier handling that preserves more of the original data distribution.\\n\\n- Age 50 is used as a significant threshold for diabetes risk stratification, dividing patients into \"mature\" and \"senior\" categories, reflecting clinical knowledge that age is a major risk factor for Type 2 diabetes with risk increasing significantly after 50.\\n\\n- The BMI categorization follows standard clinical guidelines: underweight (<18.5), healthy weight (18.5-24.9), overweight (25-29.9), and obese (â¥30), applying established medical knowledge to the feature engineering process.\\n\\n- Glucose levels are categorized using clinical thresholds that align with diabetes diagnosis criteria: normal (<140 mg/dL), prediabetes (140-199 mg/dL), and diabetes (â¥200 mg/dL).\\n\\n- The creation of interaction features between age and BMI/glucose reflects the medical understanding that these factors don\\'t operate independently but have compounding effects on diabetes risk.\\n\\n- The insulin normal range is defined as 16-166, which reflects clinical knowledge about typical fasting insulin levels, with values outside this range potentially indicating insulin resistance or pancreatic dysfunction.\\n\\n- The multiplication of Glucose and Insulin values creates a feature that approximates insulin resistance, a key factor in Type 2 diabetes development, without explicitly calculating HOMA-IR (Homeostatic Model Assessment for Insulin Resistance).\\n\\n- The code uses Random Forest for classification, which is appropriate for medical data as it handles non-linear relationships, captures feature interactions, and provides feature importance measures that can be clinically interpreted.\\n\\n- The evaluation prioritizes multiple metrics beyond accuracy, including recall (sensitivity) which is particularly important in diabetes screening to minimize false negatives (missed diagnoses).\\n    ',\n",
       " 'umerhayat123-how-i-achieved-83-accuracy': '**Your Knowledge**\\n- Missing defective units are assumed to be zero, indicating an assumption that no report of defects means no defects were found, rather than treating these as unknown values requiring imputation.\\n\\n- When negotiated price is missing, the code substitutes the unit price, implying an assumption that either no negotiation occurred or it wasn\\'t recorded, rather than treating this as a data collection error.\\n\\n- The code calculates cost savings by comparing unit price to negotiated price, and clips negative values to zero, suggesting that in this domain, negotiated prices higher than unit prices are considered data errors rather than valid business scenarios.\\n\\n- The feature selection for the predictive model deliberately excludes post-hoc information like defects, delivery status, and cost savings, focusing only on information available at or shortly after order time, which demonstrates proper handling of temporal data to prevent data leakage.\\n\\n- The code uses stratified sampling when splitting data, indicating awareness that the compliance target variable might be imbalanced, which could bias model training if not properly addressed.\\n\\n- For categorical features, the preprocessing pipeline uses the \"ignore\" strategy for handling unknown categories, showing foresight that new suppliers or item categories might appear in production data that weren\\'t present during training.\\n\\n- The model evaluation focuses primarily on accuracy, suggesting that false positives and false negatives are considered equally important in this procurement compliance context, though the code does calculate the full classification report for more detailed analysis.\\n\\n- The code tests multiple model types ranging from simple (Logistic Regression) to complex (XGBoost), indicating an exploratory approach rather than assuming a particular algorithm would be best for this procurement compliance prediction task.\\n\\n- When calculating defect rate percentage, the code handles the edge case of zero quantity orders by assigning a 0% defect rate rather than producing a division by zero error, showing attention to data quality edge cases.\\n    ',\n",
       " 'amitsinghbhadoria0-final-qt-project-analysis': '**Your Knowledge**\\n- The code handles potential multicollinearity by checking for highly correlated features (correlation > 0.9) and removing one feature from each pair, which is a common practice to improve model stability and interpretability.\\n\\n- When encoding the payment mode, the code maps \"Prepaid\" to 0 and \"COD\" (Cash on Delivery) to 1, suggesting that COD might be considered the \"positive\" case in the context of this analysis, possibly related to higher failure rates.\\n\\n- The code creates a binary feature for coupon usage rather than using the actual coupon value, indicating that the presence of a coupon (rather than its amount) is considered the important factor for predicting the target variable.\\n\\n- The implementation includes a fallback to Ridge Regression (L2 regularization) when encountering singular matrix errors, showing awareness that the dataset might have perfect separation or multicollinearity issues that simple logistic regression cannot handle.\\n\\n- The code uses stratified sampling when splitting the data, indicating awareness that the target variable \"Failure_Flag\" might be imbalanced, and maintaining the same distribution in training and test sets is important for model evaluation.\\n\\n- McFadden\\'s Pseudo RÂ² is calculated to assess model fit, which is appropriate for logistic regression models where traditional RÂ² is not applicable, showing domain knowledge in statistical modeling evaluation.\\n\\n- The code checks for and removes constant columns before modeling, recognizing that features with no variation provide no predictive power and can cause issues in model fitting.\\n    ',\n",
       " 'aarthi93-end-to-end-ml-pipeline': \"**Your Knowledge**\\n- The code drops columns with high percentages of missing values (Alley: 93%, Pool QC: 99%, Fence: 80%, Misc Feature: 96%) rather than imputing them, suggesting these features are considered less important or reliable for predicting house prices.\\n\\n- Temporal features like 'Mo Sold' and 'Yr Sold' are dropped to prevent potential data leakage, indicating awareness that including future information could lead to overfitting or unrealistic model performance.\\n\\n- The feature engineering approach creates age-related variables (House_Age, Years_Since_Remodel) using a hardcoded current year (2025), suggesting the model assumes these time-based features are important for housing price prediction.\\n\\n- The code weights half bathrooms as 0.5 of a full bathroom when calculating the total bathroom count, reflecting domain knowledge about how the real estate market values different bathroom types.\\n\\n- The preprocessing strategy differs between numeric and categorical features - numeric features use median imputation and scaling, while categorical features use constant imputation ('None') and one-hot encoding, showing awareness of appropriate transformations for different data types.\\n\\n- The Random Forest model is configured with specific hyperparameters (n_estimators=150, max_depth=30, min_samples_split=5) without apparent cross-validation, suggesting these values may be based on prior experience or domain knowledge about housing price prediction.\\n\\n- The evaluation metrics include both absolute error measures (RMSE, MAE) and a relative measure (RÂ²), indicating an understanding that different stakeholders may need different perspectives on model performance - absolute dollar errors for practical applications and RÂ² for statistical interpretation.\\n\\n- The code creates a binary feature 'Has_Pool' instead of using the continuous 'Pool Area', suggesting that the mere presence of a pool may be more predictive of house price than its specific size.\\n    \",\n",
       " 'ak5047-australia-weather': \"**Your Knowledge**\\n- The code identifies columns with >30% missing data ('Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am') and creates missing indicators rather than simply dropping them, recognizing that missingness itself may be informative for weather prediction.\\n\\n- Missing value imputation is performed using location and month-based grouping, acknowledging that weather patterns have strong geographic and seasonal dependencies.\\n\\n- The code treats 'RainToday' as a feature to be excluded from the prediction model rather than using it to predict 'RainTomorrow', suggesting an intention to build a model that predicts future rain without relying on current rain status.\\n\\n- Stratified sampling is used during train-test splitting, indicating awareness that the dataset likely has imbalanced classes (rainy vs. non-rainy days), which is common in weather prediction tasks.\\n\\n- The Random Forest model is configured with class_weight='balanced', further addressing the class imbalance issue by adjusting weights inversely proportional to class frequencies.\\n\\n- The code uses a relatively high number of trees (200) in the Random Forest, suggesting a trade-off favoring model performance over training speed, appropriate for a weather prediction task where accuracy is critical.\\n\\n- The evaluation includes precision, recall, and F1-score metrics beyond just accuracy, indicating an understanding that in weather prediction (particularly rain forecasting), false positives and false negatives may have different practical implications.\\n\\n- The code creates date-based features (Year, Month, Day) but doesn't include more complex cyclical encodings or lag features, suggesting a basic approach to temporal patterns that could potentially be enhanced.\\n    \",\n",
       " 'jakubkrasuski-store-sales-forecasting-modeling-with-lightgbm': \"**Your Knowledge**\\n- The code uses forward fill for missing oil prices rather than mean or median imputation, suggesting that the most recent oil price is a better predictor than a historical average, which aligns with the time series nature of the data.\\n\\n- Creating group-level statistics (mean, std, min, max) per store-family combination indicates that sales patterns vary significantly across different product families and stores, and these baseline statistics provide important context for the model.\\n\\n- The implementation of cyclical features for month and day of week (using sine and cosine transformations) acknowledges that these time variables have a circular nature where December (month 12) is closer to January (month 1) than to November (month 11).\\n\\n- The choice of a time-based validation split instead of random sampling respects the temporal nature of the data and prevents data leakage, as using future data to predict past events would create an unrealistic evaluation scenario.\\n\\n- Clipping negative predictions to zero reflects domain knowledge that sales quantities cannot be negative, ensuring that the model's outputs align with real-world constraints.\\n\\n- The use of RMSLE (Root Mean Squared Logarithmic Error) as an evaluation metric suggests that the relative error is more important than absolute error, which is common in sales forecasting where the percentage difference matters more than the absolute difference.\\n\\n- The creation of a binary holiday flag indicates domain knowledge that consumer purchasing behavior changes significantly on holidays, making this a potentially important predictor for retail sales.\\n\\n- The weekend indicator feature reflects the understanding that shopping patterns typically differ between weekdays and weekends, capturing weekly seasonality in retail sales.\\n\\n- The inclusion of oil prices as a feature suggests domain knowledge that economic indicators like oil prices may impact consumer purchasing power or behavior, especially relevant in countries where the economy is sensitive to oil prices.\\n\\n- The model uses LightGBM, a gradient boosting framework that handles categorical features efficiently and performs well on tabular data, indicating a deliberate choice based on the structured nature of the store sales data.\\n    \",\n",
       " 'abdallaellaithy-titanic-in-space-ml-survival-predictions': \"**Your Knowledge**\\n- The code treats the Spaceship Titanic problem as a binary classification task, suggesting that predicting passenger transportation is best approached as a probability of an event occurring rather than a regression problem.\\n\\n- Cabin information is parsed into three components (Deck, Number, Side), indicating that the spatial location within the ship may have different influences on the target variable, rather than treating the cabin as a single categorical entity.\\n\\n- The creation of 'PassengerGroup' and 'IsAlone' features suggests that traveling companions may influence transportation outcomes, reflecting a domain understanding that group dynamics could affect survival or selection.\\n\\n- Spending features are treated specially by summing them into 'TotalSpend' and creating a binary 'HasSpent' feature, indicating that the pattern of spending (whether a passenger spent anything at all) may be more informative than the exact amounts.\\n\\n- Missing values in spending columns are filled with zeros rather than using imputation techniques, suggesting a domain assumption that missing spending values likely indicate no spending rather than missing data.\\n\\n- The code uses ROC AUC as the primary metric for model selection and hyperparameter tuning rather than accuracy, indicating that the balance between true positive and false positive rates is more important than raw prediction correctness, possibly due to class imbalance.\\n\\n- The model evaluation strategy uses stratified sampling for both train-test splitting and cross-validation, preserving the original class distribution and ensuring that model performance isn't artificially inflated by predicting the majority class.\\n\\n- The hyperparameter tuning focuses on tree-based models with parameters that control model complexity (depth, samples per split) and ensemble strength (number of estimators, learning rate), suggesting a balance is needed between fitting the training data and generalizing to new data.\\n\\n- The final predictions are converted to boolean type before submission, indicating a requirement of the competition format and ensuring compatibility with the expected output schema.\\n    \",\n",
       " 'hanymato-mobile-price-prediction-model': '**Your Knowledge**\\n- The code handles missing values in two stages: first replacing all with zeros, then applying mean imputation for numerical columns, suggesting a strategy to avoid errors while preserving as much data as possible rather than dropping incomplete records.\\n\\n- When processing RAM and screen size specifications, the code extracts the maximum value when multiple options are present (e.g., \"8GB/12GB\" RAM), indicating an assumption that the highest specification is most relevant for price prediction.\\n\\n- The camera processing logic prioritizes megapixel count as the primary indicator of camera quality, ignoring other potentially relevant specifications like aperture, sensor size, or video capabilities.\\n\\n- The decision to use Label Encoding rather than One-Hot Encoding for the \"Company Name\" feature suggests an assumption that there\\'s an ordinal relationship between manufacturers that might correlate with price.\\n\\n- The selection of features (RAM, Battery Capacity, Screen Size, Front Camera, Back Camera, Company Name) reflects domain knowledge about which specifications most strongly influence mobile phone pricing.\\n\\n- The choice of a Decision Tree Regressor without hyperparameter tuning indicates this may be an initial baseline model, as decision trees can capture non-linear relationships but are prone to overfitting without proper regularization.\\n\\n- The code drops rows with missing target values after the train-test split rather than before, which preserves the original data distribution in the split but potentially reduces the effective training set size.\\n\\n- The absence of feature scaling suggests an understanding that Decision Trees are not sensitive to feature scales, unlike many other machine learning algorithms that would require normalization.\\n    '}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Create a comprehensive feature engineering function that:\\n1. Extracts titles from passenger names and groups rare titles\\n2. Creates family-related features (family size, is alone flag, family group categories)\\n3. Processes cabin information (has cabin flag, deck extraction)\\n4. Handles ticket information (prefix extraction, numeric conversion)\\n5. Bins age and fare into categorical groups\\n6. Creates interaction features (Age*Class, Fare per person)\\nThen apply this function to both training and test datasets.\\n\\n- Handle missing values in the datasets by:\\n1. Filling missing Age values based on Title, Sex, and Pclass group medians\\n2. Imputing missing Embarked values with the mode\\n3. Filling missing Fare values in the test set with median fares by Pclass\\n4. Recalculating age and fare bins after imputation to ensure consistency\\n\\n- Prepare the data for modeling by:\\n1. Selecting relevant features for prediction\\n2. Separating features into numeric and categorical groups\\n3. Creating preprocessing pipelines with appropriate imputation and scaling/encoding\\n4. Setting up a ColumnTransformer to apply the right transformations to each feature type\\n\\n- Build and evaluate multiple machine learning models by:\\n1. Creating a dictionary of models (Logistic Regression, Random Forest, Gradient Boosting, SVC)\\n2. Setting up pipelines that combine preprocessing with each classifier\\n3. Defining a cross-validation strategy using StratifiedKFold\\n4. Evaluating each model using cross-validation and storing the results\\n\\n- Perform hyperparameter tuning for the top-performing models by:\\n1. Defining parameter grids for each model type\\n2. Using GridSearchCV with cross-validation to find optimal parameters\\n3. Selecting the best models based on cross-validation performance\\n\\n- Create an ensemble model by:\\n1. Combining the best-tuned models into a VotingClassifier with soft voting\\n2. Training the ensemble on the full training dataset\\n3. Evaluating the ensemble using cross-validation\\n4. Comparing the ensemble performance with individual model performance\\n\\n- Generate predictions and create a submission file by:\\n1. Using the trained ensemble model to predict survival on the test dataset\\n2. Creating a submission dataframe with PassengerId and predicted Survived values\\n3. Saving the predictions to a CSV file for submission\\n    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
