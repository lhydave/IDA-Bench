# Sample LLM configuration with enhanced RPM control

# Required configuration
api_key = "your-api-key-here"
model = "gpt-3.5-turbo"

# Optional parameters with defaults
temperature = 0.4
max_retries = 3
retry_delay = 2
run_code = true
api_base = "https://api.openai.com/v1"  # Optional custom API endpoint
checkpoint_path = "./checkpoints/conversation.json"

# Rate limiting options
rpm = 60  # Default requests per minute limit
adaptive_rate_limit = true  # Automatically adjust rate limits based on API responses

# Model-specific RPM limits
# This will override the default RPM for specific models
[rpm_per_model]
"gpt-3.5-turbo" = 60
"gpt-4" = 30
"gpt-4-turbo" = 40
"claude-3-sonnet" = 20 
