{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will copy all necessary information from preprocessing_notebook to benchmark_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def create_directory_structure(base_folder, folder_name):\n",
    "    \"\"\"Create the required directory structure in benchmark_final/storage.\"\"\"\n",
    "    storage_path = Path(\"benchmark_final/storage\") / folder_name\n",
    "    for subfolder in [\"datasets\", \"instructions\", \"ground_truth\", \"evaluation\"]:\n",
    "        (storage_path / subfolder).mkdir(parents=True, exist_ok=True)\n",
    "    return storage_path\n",
    "\n",
    "def get_data_files(folder_path):\n",
    "    \"\"\"Get the data files from the dataset folder.\"\"\"\n",
    "    dataset_path = Path(folder_path) / \"dataset\"\n",
    "    if not dataset_path.exists():\n",
    "        return [], []\n",
    "    \n",
    "    data_for_agent = []\n",
    "    data_ground_truth = []\n",
    "    \n",
    "    for file in dataset_path.glob(\"*\"):\n",
    "        if file.is_file():\n",
    "            if \"baseline_submission\" in file.name:\n",
    "                continue\n",
    "            elif \"groundtruth_df\" in file.name:\n",
    "                data_ground_truth.append(file)\n",
    "            else:\n",
    "                data_for_agent.append(file)\n",
    "    \n",
    "    return data_for_agent, data_ground_truth\n",
    "\n",
    "def get_knowledge(folder_path):\n",
    "    \"\"\"Read the knowledge.md file.\"\"\"\n",
    "    knowledge_path = Path(folder_path) / \"knowledge.md\"\n",
    "    assert knowledge_path.exists()\n",
    "    return knowledge_path.read_text()\n",
    "\n",
    "def get_instructions(folder_path):\n",
    "    \"\"\"Read the instructions.md file.\"\"\"\n",
    "    instructions_path = Path(folder_path) / \"instructions.md\"\n",
    "    assert instructions_path.exists()\n",
    "    return instructions_path.read_text()\n",
    "\n",
    "def get_sample_submission_columns(folder_path):\n",
    "    \"\"\"Get column names from sample_submission.csv file.\"\"\"\n",
    "    sample_submission_path = Path(folder_path) / \"dataset\" / \"sample_submission.csv\"\n",
    "    print(sample_submission_path)\n",
    "    if not sample_submission_path.exists():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(sample_submission_path)\n",
    "        return list(df.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading sample submission file: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_objective(folder_path):\n",
    "    \"\"\"Read the objective.md file and replace column names placeholder.\"\"\"\n",
    "    objective_path = Path(os.path.dirname(folder_path)) / \"objective.md\"\n",
    "    assert objective_path.exists()\n",
    "    \n",
    "    objective_content = objective_path.read_text()\n",
    "    \n",
    "    # Get column names from sample submission\n",
    "    column_names = get_sample_submission_columns(folder_path)\n",
    "    print(column_names)\n",
    "    if column_names:\n",
    "        # Replace the placeholder with actual column names\n",
    "        print(objective_content)\n",
    "        objective_content = objective_content.replace(\"{column_names}\", \", \".join(column_names))\n",
    "    \n",
    "    return objective_content\n",
    "\n",
    "def create_instructions_file(storage_path, objective, knowledge):\n",
    "    \"\"\"Create the instructions.md file with the specified structure.\"\"\"\n",
    "    instructions_path = storage_path / \"instructions\" / \"instructions.md\"\n",
    "    content = f\"\"\"**Objective**\\n{objective}\\n\\n**Your Knowledge**\\n{knowledge}\n",
    "    \"\"\"\n",
    "    instructions_path.write_text(content)\n",
    "\n",
    "def create_gatekeeper_reference(storage_path, objective, instructions):\n",
    "    \"\"\"Read the knowledge.md file.\"\"\"\n",
    "    gatekeeper_reference_path = storage_path / \"instructions\" / \"gatekeeper_reference.md\"\n",
    "    content = f\"\"\"- {objective}\\n\\n{instructions}\n",
    "    \"\"\"\n",
    "    gatekeeper_reference_path.write_text(content)\n",
    "\n",
    "def create_meta_info_file(storage_path, folder_name, data_files):\n",
    "    \"\"\"Create a meta info JSON file for the benchmark.\"\"\"\n",
    "    meta_info = {\n",
    "        \"notebook_id\": folder_name,\n",
    "        \"input_ids\": [file.name for file in data_files],\n",
    "        \"num_rounds\": None\n",
    "    }\n",
    "    \n",
    "    # Create meta_info directory if it doesn't exist\n",
    "    meta_info_dir = Path(\"benchmark_final/meta_info/storage\")\n",
    "    meta_info_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Write meta info to JSON file\n",
    "    meta_info_path = meta_info_dir / f\"{folder_name}.json\"\n",
    "    with open(meta_info_path, 'w') as f:\n",
    "        json.dump(meta_info, f, indent=4)\n",
    "    \n",
    "    return meta_info\n",
    "\n",
    "def update_benchmark_list(benchmark_info):\n",
    "    \"\"\"Update the benchmark_list.json file with new benchmark information.\"\"\"\n",
    "    benchmark_list_path = Path(\"benchmark_final/meta_info/benchmark_list.json\")\n",
    "    \n",
    "    # Load existing benchmark list if it exists\n",
    "    if benchmark_list_path.exists():\n",
    "        with open(benchmark_list_path, 'r') as f:\n",
    "            benchmark_list = json.load(f)\n",
    "    else:\n",
    "        benchmark_list = []\n",
    "    \n",
    "    # Add new benchmark if it doesn't exist\n",
    "    if benchmark_info[\"notebook_id\"] not in benchmark_list:\n",
    "        benchmark_list.append(benchmark_info[\"notebook_id\"])\n",
    "    \n",
    "    # Write updated list back to file\n",
    "    with open(benchmark_list_path, 'w') as f:\n",
    "        json.dump(benchmark_list, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_folder(folder_path, llm_callback):\n",
    "    \"\"\"Process a single folder and organize its contents.\"\"\"\n",
    "    folder_name = Path(folder_path).name.replace(\"#####\", \"-\")\n",
    "    \n",
    "    # Create directory structure\n",
    "    storage_path = create_directory_structure(\"benchmark_final/storage\", folder_name)\n",
    "    \n",
    "    # Get data files\n",
    "    data_for_agent, data_ground_truth = get_data_files(folder_path)\n",
    "    \n",
    "    # Get knowledge\n",
    "    knowledge = get_knowledge(folder_path)\n",
    "    \n",
    "    objective = get_objective(folder_path)\n",
    "\n",
    "    instructions = get_instructions(folder_path)\n",
    "    \n",
    "    # Copy files to appropriate locations\n",
    "    for file in data_for_agent:\n",
    "        shutil.copy2(file, storage_path / \"datasets\" / file.name)\n",
    "    \n",
    "    for file in data_ground_truth:\n",
    "        shutil.copy2(file, storage_path / \"ground_truth\" / file.name)\n",
    "\n",
    "    shutil.copy2(os.path.join(folder_path, \"evaluation_metrics.py\"), storage_path / \"evaluation\" / \"evaluation_metrics.py\")\n",
    "    shutil.copy2(os.path.join(folder_path, \"numeric_baseline.json\"), storage_path / \"evaluation\" / \"numeric_baseline.json\")\n",
    "    \n",
    "    # Create instructions file\n",
    "    create_instructions_file(storage_path, objective, knowledge)\n",
    "    create_gatekeeper_reference( storage_path, objective, instructions)\n",
    "\n",
    "    # Create meta info file and update benchmark list\n",
    "    meta_info = create_meta_info_file(storage_path, folder_name, data_for_agent)\n",
    "    update_benchmark_list(meta_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing umerhayat123#####how-i-achieved-83-accuracy...\n",
      "Processing hanymato#####mobile-price-prediction-model...\n",
      "Processing drpashamd4r#####indian-floods-data-exploratory...\n",
      "Processing patilaakash619#####backpack-price-prediction-ml-guide...\n",
      "preprocessing_notebook/preprocess_data/patilaakash619#####backpack-price-prediction-ml-guide/dataset/sample_submission.csv\n",
      "['id', 'Price']\n",
      "Your goal is to analyze and create a submission file using the provided dataset. The dataset contains csv files saved at '/app/datasets'. The data is split into '/app/datasets/train.csv' (with targets) for training, and '/app/datasets/test.csv' (without targets) for evaluation. Based on your model's predictions for the test set, create a submission.csv file matching the format shown in '/app/datasets/sample_submission.csv'. Specifically, the column names in the submission file should be \"{column_names}\". The csv file **must** be submitted at \"/app/checkpoints/submission.csv\".\n",
      "Processing amitsinghbhadoria0#####final-qt-project-analysis...\n",
      "Processing mightyjiraiya#####titanic-survival-prediction...\n",
      "Processing hasangulec#####feature-engineering-diabetes...\n",
      "Processing dmytrobuhai#####eda-rf...\n",
      "Processing ayodejiibrahimlateef#####integrative-analysis-early-depression-detection...\n",
      "Processing patilaakash619#####electric-vehicle-population-data-in-the-us...\n",
      "Processing abdallaellaithy#####titanic-in-space-ml-survival-predictions...\n",
      "Processing aarthi93#####end-to-end-ml-pipeline...\n",
      "Processing jakubkrasuski#####store-sales-forecasting-modeling-with-lightgbm...\n",
      "Processing esotericdata1#####titanickaggle-ds...\n",
      "Processing walmart-sales-forecasting...\n",
      "Processing iseedeep#####mission-podcast-listening-prediction...\n",
      "Processing sasakitetsuya#####predicting-startup-valuation-with-machine-learning...\n",
      "Processing ugurcan95#####brazilian-tweet-sentiment-analysis...\n",
      "Processing jakubkrasuski#####llm-chatbot-arena-predicting-user-preferences...\n",
      "Processing ak5047#####australia-weather...\n",
      "Processing vijaythurimella#####bank-subscriptions-predictions-f1-score...\n",
      "Processing shaswatatripathy#####store-sales-prediction...\n",
      "preprocessing_notebook/preprocess_data/shaswatatripathy#####store-sales-prediction/dataset/sample_submission.csv\n",
      "['id', 'sales']\n",
      "Your goal is to analyze and create a submission file using the provided dataset. The dataset contains csv files saved at '/app/datasets'. The data is split into '/app/datasets/train.csv' (with targets) for training, and '/app/datasets/test.csv' (without targets) for evaluation. Based on your model's predictions for the test set, create a submission.csv file matching the format shown in '/app/datasets/sample_submission.csv'. Specifically, the column names in the submission file should be \"{column_names}\". The csv file **must** be submitted at \"/app/checkpoints/submission.csv\".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_path = Path(\"preprocessing_notebook/preprocess_data\")\n",
    "\n",
    "# Create benchmark_final/storage directory if it doesn't exist\n",
    "Path(\"benchmark_final/storage\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Process each folder\n",
    "for folder in base_path.iterdir():\n",
    "    if folder.is_dir() and not folder.name.startswith('.'):\n",
    "        print(f\"Processing {folder.name}...\")\n",
    "        if folder.name not in [\"patilaakash619#####backpack-price-prediction-ml-guide\", \"shaswatatripathy#####store-sales-prediction\"]:\n",
    "            continue\n",
    "        # You'll need to implement the llm_callback function\n",
    "        process_folder(folder, lambda x: \"PLACEHOLDER OBJECTIVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following content is a \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
