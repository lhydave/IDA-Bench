{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timestamp_from_filename(filename):\n",
    "    \"\"\"Extract timestamp from filename.\n",
    "    Expected format: *_YYYYMMDD_HHMMSS.json\"\"\"\n",
    "    match = re.search(r'_(\\d{8}_\\d{6})\\.json$', filename)\n",
    "    if match:\n",
    "        timestamp_str = match.group(1)\n",
    "        return timestamp_str\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_failed(result_dict):\n",
    "    \"\"\"Check if a result represents a failed run.\n",
    "    Failed if skill_score is -10000, nan, or missing.\"\"\"\n",
    "    try:\n",
    "        skill_score = result_dict.get('metrics', {}).get('skill_score')\n",
    "        return (skill_score == -10000 or \n",
    "                (isinstance(skill_score, float) and np.isnan(skill_score)) or\n",
    "                skill_score is None)\n",
    "    except:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results list\n",
    "all_results = []\n",
    "\n",
    "# Process all JSON files in the results directory\n",
    "results_dir = 'experiments/results'\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith('.json') and filename != 'summary.json' and filename != 'evaluation_only_summary.json':\n",
    "        file_path = os.path.join(results_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                result_dict = json.load(f)\n",
    "                \n",
    "            # Add timestamp and failed status\n",
    "            timestamp = parse_timestamp_from_filename(filename)\n",
    "            if timestamp:\n",
    "                result_dict['timestamp'] = timestamp\n",
    "                result_dict['failed'] = is_failed(result_dict)\n",
    "                all_results.append(result_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results processed: 175\n",
      "\n",
      "Sample result:\n",
      "{\n",
      "  \"benchmark_id\": \"ugurcan95-brazilian-tweet-sentiment-analysis\",\n",
      "  \"agent_id\": \"litellm_proxy/deepseek-r1\",\n",
      "  \"completion_status\": \"completed\",\n",
      "  \"metrics\": {\n",
      "    \"interaction_time_seconds\": 505.785724,\n",
      "    \"conversation_turns\": 8,\n",
      "    \"code_snippets_count\": 5,\n",
      "    \"total_code_executions\": 5,\n",
      "    \"code_operations\": {\n",
      "      \"pandas_operations\": 10,\n",
      "      \"plotting\": 0,\n",
      "      \"dataframe_creation\": 0,\n",
      "      \"file_io\": 4,\n",
      "      \"error_handling\": 0,\n",
      "      \"loops\": 3,\n",
      "      \"functions\": 3,\n",
      "      \"imports\": 22,\n",
      "      \"error_count\": 2\n",
      "    },\n",
      "    \"absolute_metric_score\": 0.7493333333333333,\n",
      "    \"skill_score\": 0.020067761271826905\n",
      "  },\n",
      "  \"timestamp\": \"20250513_144422\",\n",
      "  \"failed\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Display the number of results processed\n",
    "print(f\"Total results processed: {len(all_results)}\")\n",
    "\n",
    "# Display a sample result\n",
    "if all_results:\n",
    "    print(\"\\nSample result:\")\n",
    "    print(json.dumps(all_results[0], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find out competition/non-competition notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 competition benchmarks:\n",
      "- vijaythurimella-bank-subscriptions-predictions-f1-score\n",
      "- patilaakash619-backpack-price-prediction-ml-guide\n",
      "- esotericdata1-titanickaggle-ds\n",
      "- dmytrobuhai-eda-rf\n",
      "- jakubkrasuski-llm-chatbot-arena-predicting-user-preferences\n",
      "- ugurcan95-brazilian-tweet-sentiment-analysis\n",
      "- abdallaellaithy-titanic-in-space-ml-survival-predictions\n",
      "- shaswatatripathy-store-sales-prediction\n",
      "- jakubkrasuski-store-sales-forecasting-modeling-with-lightgbm\n",
      "- mightyjiraiya-titanic-survival-prediction\n",
      "- iseedeep-mission-podcast-listening-prediction\n",
      "\n",
      "Found 15 non-competition benchmarks:\n",
      "- patilaakash619-electric-vehicle-population-data-in-the-us\n",
      "- sasakitetsuya-predicting-startup-valuation-with-machine-learning\n",
      "- hanymato-mobile-price-prediction-model\n",
      "- sudalairajkumar-simple-feature-engg-notebook-spooky-author\n",
      "- hasangulec-feature-engineering-diabetes\n",
      "- jimmyyeung-spaceship-titanic-xgb-top5\n",
      "- drpashamd4r-indian-floods-data-exploratory\n",
      "- aarthi93-end-to-end-ml-pipeline\n",
      "- umerhayat123-how-i-achieved-83-accuracy\n",
      "- slimreaper-random-forest-xgb-catboost-ensemble-t40\n",
      "- tetsutani-ps3e9-eda-and-gbdt-catboost-median-duplicatedata\n",
      "- mohitsital-top-10-bike-sharing-rf-gbm\n",
      "- ayodejiibrahimlateef-integrative-analysis-early-depression-detection\n",
      "- amitsinghbhadoria0-final-qt-project-analysis\n",
      "- ak5047-australia-weather\n"
     ]
    }
   ],
   "source": [
    "# Get all benchmark IDs from the notebook files\n",
    "competition_notebooks_dir = \"benchmark_data_toSubmit/notebooks/storage\"\n",
    "competition_benchmark = []\n",
    "\n",
    "# Pattern to convert filenames to benchmark IDs\n",
    "# Replace ##### with - in filenames\n",
    "notebook_files = os.listdir(competition_notebooks_dir)\n",
    "for notebook_file in notebook_files:\n",
    "    if notebook_file.endswith('.ipynb'):\n",
    "        # Convert filename to benchmark_id (replace ##### with -)\n",
    "        benchmark_id = notebook_file[:-6].replace('#####', '-')\n",
    "        competition_benchmark.append(benchmark_id)\n",
    "\n",
    "# Print the competition benchmarks\n",
    "print(f\"Found {len(competition_benchmark)} competition benchmarks:\")\n",
    "for benchmark in competition_benchmark:\n",
    "    print(f\"- {benchmark}\")\n",
    "\n",
    "# Create the noncompetition_benchmark list\n",
    "# This includes benchmark_ids in all_results that aren't in competition_benchmark\n",
    "noncompetition_benchmark = []\n",
    "\n",
    "# Get unique benchmark_ids from all_results\n",
    "all_benchmark_ids = set()\n",
    "for result in all_results:\n",
    "    benchmark_id = result.get('benchmark_id')\n",
    "    if benchmark_id:\n",
    "        all_benchmark_ids.add(benchmark_id)\n",
    "\n",
    "# Find benchmarks that aren't in the competition list\n",
    "for benchmark_id in all_benchmark_ids:\n",
    "    if benchmark_id not in competition_benchmark:\n",
    "        noncompetition_benchmark.append(benchmark_id)\n",
    "\n",
    "# Print the non-competition benchmarks\n",
    "print(f\"\\nFound {len(noncompetition_benchmark)} non-competition benchmarks:\")\n",
    "for benchmark in noncompetition_benchmark:\n",
    "    print(f\"- {benchmark}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_benchmark_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find out the latest result for one agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 latest results\n",
      "\n",
      "Sample latest result:\n",
      "{\n",
      "  \"benchmark_id\": \"jakubkrasuski-llm-chatbot-arena-predicting-user-preferences\",\n",
      "  \"agent_id\": \"litellm_proxy/deepseek-v3\",\n",
      "  \"completion_status\": \"completed\",\n",
      "  \"metrics\": {\n",
      "    \"interaction_time_seconds\": 1434.908057,\n",
      "    \"conversation_turns\": 13,\n",
      "    \"code_snippets_count\": 34,\n",
      "    \"total_code_executions\": 36,\n",
      "    \"code_operations\": {\n",
      "      \"pandas_operations\": 30,\n",
      "      \"plotting\": 16,\n",
      "      \"dataframe_creation\": 5,\n",
      "      \"file_io\": 29,\n",
      "      \"error_handling\": 7,\n",
      "      \"loops\": 28,\n",
      "      \"functions\": 10,\n",
      "      \"imports\": 97,\n",
      "      \"error_count\": 5\n",
      "    },\n",
      "    \"absolute_metric_score\": 11.633621698622319,\n",
      "    \"skill_score\": -9.671527750281275\n",
      "  },\n",
      "  \"timestamp\": \"20250514_153030\",\n",
      "  \"failed\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def find_latest_result_for_agent(agent_id):\n",
    "    \"\"\"\n",
    "    Find the latest result for each benchmark_id for a given agent_id.\n",
    "    \n",
    "    Args:\n",
    "        agent_id (str): The ID of the agent to search for\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the latest results for each benchmark\n",
    "    \"\"\"\n",
    "    # Dictionary to store the latest result for each benchmark_id\n",
    "    latest_results = {}\n",
    "    \n",
    "    # Iterate through all results\n",
    "    for result in all_results:\n",
    "        # Skip if this result is not from the specified agent\n",
    "        if result.get('agent_id') != agent_id:\n",
    "            continue\n",
    "            \n",
    "        benchmark_id = result.get('benchmark_id')\n",
    "        if not benchmark_id:\n",
    "            continue\n",
    "            \n",
    "        # If we haven't seen this benchmark_id before, or if this result is newer\n",
    "        if (benchmark_id not in latest_results or \n",
    "            result['timestamp'] > latest_results[benchmark_id]['timestamp']):\n",
    "            if result['timestamp'] != '20250514_153030': # HACK: only for new prompt\n",
    "                continue\n",
    "            latest_results[benchmark_id] = result\n",
    "    \n",
    "    # Convert the dictionary values to a list\n",
    "    return list(latest_results.values())\n",
    "\n",
    "# Example usage:\n",
    "latest_results = find_latest_result_for_agent(\"litellm_proxy/deepseek-v3\")\n",
    "print(f\"Found {len(latest_results)} latest results\")\n",
    "if latest_results:\n",
    "    print(\"\\nSample latest result:\")\n",
    "    print(json.dumps(latest_results[0], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_v3_latest_results = find_latest_result_for_agent(\"litellm_proxy/deepseek-v3\")\n",
    "deepseek_r1_latest_results = find_latest_result_for_agent(\"litellm_proxy/deepseek-r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deepseek_v3_latest_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deepseek_r1_latest_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion of positive skill scores\n",
      "deepseek-v3:  0.11538461538461539\n",
      "deepseek-r1:  0.3076923076923077\n",
      "\n",
      "\n",
      "proportion of failed runs\n",
      "deepseek-v3:  0.6153846153846154\n",
      "deepseek-r1:  0.38461538461538464\n",
      "\n",
      "\n",
      "average skill score of non-failed runs\n",
      "deepseek-v3:  -1.1120553481682456\n",
      "deepseek-r1:  -1.6452219027036477\n",
      "\n",
      "\n",
      "proportion of positive skill scores among non-failed runs\n",
      "deepseek-v3:  0.3\n",
      "deepseek-r1:  0.5\n",
      "\n",
      "\n",
      "proportion of positive skill scores among non-competition benchmarks\n",
      "deepseek-v3:  0.0\n",
      "deepseek-r1:  0.2\n",
      "\n",
      "\n",
      "proportion of positive skill scores among competition benchmarks\n",
      "deepseek-v3:  0.2727272727272727\n",
      "deepseek-r1:  0.45454545454545453\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deepseek_v3_is_failed = np.array([i[\"failed\"] for i in deepseek_v3_latest_results])\n",
    "deepseek_r1_is_failed = np.array([i[\"failed\"] for i in deepseek_r1_latest_results])\n",
    "deepseek_v3_skill_scores = np.array([i[\"metrics\"][\"skill_score\"] for i in deepseek_v3_latest_results])\n",
    "deepseek_r1_skill_scores = np.array([i[\"metrics\"][\"skill_score\"] for i in deepseek_r1_latest_results])\n",
    "deepseek_v3_skill_scores.shape\n",
    "deepseek_r1_skill_scores.shape\n",
    "\n",
    "# proportion of positive skill scores\n",
    "print(\"proportion of positive skill scores\")\n",
    "print(\"deepseek-v3: \",np.sum(deepseek_v3_skill_scores >= -0.05) / len(deepseek_v3_skill_scores))\n",
    "print(\"deepseek-r1: \",np.sum(deepseek_r1_skill_scores >= -0.05) / len(deepseek_r1_skill_scores))\n",
    "print(\"\\n\")\n",
    "\n",
    "# proportion of failed runs\n",
    "print(\"proportion of failed runs\")\n",
    "print(\"deepseek-v3: \",np.sum(deepseek_v3_is_failed) / len(deepseek_v3_is_failed))\n",
    "print(\"deepseek-r1: \",np.sum(deepseek_r1_is_failed) / len(deepseek_r1_is_failed))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"average skill score of non-failed runs\")\n",
    "print(\"deepseek-v3: \",np.mean(deepseek_v3_skill_scores[~deepseek_v3_is_failed]))\n",
    "print(\"deepseek-r1: \",np.mean(deepseek_r1_skill_scores[~deepseek_r1_is_failed]))\n",
    "print(\"\\n\")\n",
    "\n",
    "# proportion of positive skill scores among non-failed runs\n",
    "print(\"proportion of positive skill scores among non-failed runs\")\n",
    "print(\"deepseek-v3: \", np.sum(deepseek_v3_skill_scores[~deepseek_v3_is_failed] >= -0.05) / len(deepseek_v3_skill_scores[~deepseek_v3_is_failed]))\n",
    "print(\"deepseek-r1: \", np.sum(deepseek_r1_skill_scores[~deepseek_r1_is_failed] >= -0.05) / len(deepseek_r1_skill_scores[~deepseek_r1_is_failed]))\n",
    "print(\"\\n\")\n",
    "\n",
    "# proportion of positive skill scores among non-competition benchmarks\n",
    "print(\"proportion of positive skill scores among non-competition benchmarks\")\n",
    "deepseek_v3_skill_scores_non_competition = deepseek_v3_skill_scores[np.array([i[\"benchmark_id\"] not in competition_benchmark for i in deepseek_v3_latest_results])]\n",
    "deepseek_r1_skill_scores_non_competition = deepseek_r1_skill_scores[np.array([i[\"benchmark_id\"] not in competition_benchmark for i in deepseek_r1_latest_results])]\n",
    "print(\"deepseek-v3: \", np.sum(deepseek_v3_skill_scores_non_competition >= -0.05) / len(deepseek_v3_skill_scores_non_competition))\n",
    "print(\"deepseek-r1: \", np.sum(deepseek_r1_skill_scores_non_competition >= -0.05) / len(deepseek_r1_skill_scores_non_competition))\n",
    "print(\"\\n\")\n",
    "\n",
    "# proportion of positive skill scores among non-competition benchmarks\n",
    "print(\"proportion of positive skill scores among competition benchmarks\")\n",
    "deepseek_v3_skill_scores_competition = deepseek_v3_skill_scores[np.array([i[\"benchmark_id\"] in competition_benchmark for i in deepseek_v3_latest_results])]\n",
    "deepseek_r1_skill_scores_competition = deepseek_r1_skill_scores[np.array([i[\"benchmark_id\"] in competition_benchmark for i in deepseek_r1_latest_results])]\n",
    "print(\"deepseek-v3: \", np.sum(deepseek_v3_skill_scores_competition >= -0.05) / len(deepseek_v3_skill_scores_competition))\n",
    "print(\"deepseek-r1: \", np.sum(deepseek_r1_skill_scores_competition >= -0.05) / len(deepseek_r1_skill_scores_competition))\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00000000e+04,  5.19492346e-04,  3.72050817e-02,  4.90654530e-04,\n",
       "        4.48448333e-02,  7.10923041e-01,  3.92156863e-02, -1.00000000e+04,\n",
       "       -8.62068966e-02, -1.00000000e+04, -4.25490196e+00,  4.34782609e-02,\n",
       "       -1.00000000e+04, -1.00000000e+04])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepseek_r1_skill_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00000000e+04,  5.19492346e-04,  3.72050817e-02,  4.90654530e-04,\n",
       "        7.10923041e-01, -4.25490196e+00,  4.34782609e-02])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepseek_r1_skill_scores_competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False,  True, False,\n",
       "        True, False, False, False, False, False, False,  True,  True,\n",
       "       False])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepseek_v3_is_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = [i for i in all_results if i['agent_id'] == 'litellm_proxy/deepseek-v3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
